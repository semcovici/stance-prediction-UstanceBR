{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from unidecode import unidecode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_data = '../data/raw/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Imports\n",
    "######################\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Definitions\n",
    "######################\n",
    "\n",
    "L = 15\n",
    "\n",
    "path_raw_data = '../data/raw/'\n",
    "path_processed_data = '../data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Lists\n",
    "######################\n",
    "terms_list_ig = [\n",
    "    \"vaticano\",\n",
    "    \"crisma\",\n",
    "    \"comunhão\",\n",
    "    \"batismo\",\n",
    "    \"culto\",\n",
    "    \"missa\",\n",
    "    \"hóstia\",\n",
    "    \"cálice\",\n",
    "    \"crucifixo\",\n",
    "    \"altar\",\n",
    "    \"sacerdote\",\n",
    "    \"papa\",\n",
    "    \"bispo\",\n",
    "    \"paróquia\",\n",
    "    \"templo\",\n",
    "    \"capela\",\n",
    "    \"catedral\",\n",
    "    \"pastor\",\n",
    "    \"padre\",\n",
    "    \"igreja\"]\n",
    "\n",
    "\n",
    "terms_list_cl = [\n",
    "    \"droga\",\n",
    "    \"antimalárico\",\n",
    "    \"tratamento\",\n",
    "    \"medicamento\",\n",
    "    \"remédio\",\n",
    "    \"hidroxicloroquina\",\n",
    "    \"cloroquina\"]\n",
    "\n",
    "\n",
    "terms_list_lu = [\n",
    "    \"13\",\n",
    "    \"política\",\n",
    "    \"governo\",\n",
    "    \"ex-presidente\",\n",
    "    \"luiz inácio lula da silva\",\n",
    "    \"partido dos trabalhadores\",\n",
    "    \"presidente\",\n",
    "    \"pt\",\n",
    "    \"lula\"]\n",
    "\n",
    "\n",
    "terms_list_co = [\n",
    "    \"china\",\n",
    "    \"pandemia\",\n",
    "    \"covid-19\",\n",
    "    \"biontech\",\n",
    "    \"vacinação\",\n",
    "    \"imunização\",\n",
    "    \"vacina\",\n",
    "    \"vachina\",\n",
    "    \"coronavac\",\n",
    "    \"sinovac\"]\n",
    "\n",
    "\n",
    "terms_list_gl = [\n",
    "    \"jornalismo\",\n",
    "    \"mídia\",\n",
    "    \"emissora\",\n",
    "    \"televisão\",\n",
    "    \"tv\",\n",
    "    \"globo\"]\n",
    "\n",
    "\n",
    "terms_list_bo = [\n",
    "    \"17\",\n",
    "    \"22\",\n",
    "    \"ex-presidente\",\n",
    "    \"conservador\",\n",
    "    \"política\",\n",
    "    \"pl\",\n",
    "    \"partido liberal\",\n",
    "    \"governo bolsonaro\",\n",
    "    \"presidente\",\n",
    "    \"jair\",\n",
    "    \"bolsonaro\"\n",
    "]\n",
    "\n",
    "target_terms_dict = {\n",
    "    'ig': [term.casefold() for term in terms_list_ig],\n",
    "    'bo': [term.casefold() for term in terms_list_bo], \n",
    "    'cl': [term.casefold() for term in terms_list_cl], \n",
    "    'co': [term.casefold() for term in terms_list_co], \n",
    "    'gl': [term.casefold() for term in terms_list_gl], \n",
    "    'lu': [term.casefold() for term in terms_list_lu]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Functions\n",
    "######################\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(sentences):\n",
    "    # Split sentences into lists of words\n",
    "    tokenized = np.char.split(sentences)\n",
    "    return tokenized\n",
    "# given comments separated by \" # \" and a list of terms, \n",
    "# return all coments that have at least one of terms in the terms_list\n",
    "def find_relevant_comments(comments, terms_list, L=None):\n",
    "\n",
    "    # Tokenização dos comentários\n",
    "    list_comments = np.array(comments.split(' # '))\n",
    "    tokenized_comments = tokenize_sentences(list_comments) \n",
    "    \n",
    "    func = lambda tokenized_comment: o if (o:= np.where(np.isin(terms_list, tokenized_comment) == 1)[0].max(initial = -100000)) else -1\n",
    "    vfunc = np.vectorize(func)\n",
    "    score_com = vfunc(tokenized_comments)\n",
    "    \n",
    "    sorted_score_com = sorted(zip(score_com, list_comments))\n",
    "    \n",
    "    if L is not None:\n",
    "        \n",
    "        sorted_score_com = sorted_score_com[-L:]\n",
    "    \n",
    "    #sorted_scores, sorted_com=list(zip(*sorted_score_com))      \n",
    "    # Concatenação dos comentários relevantes\n",
    "    #str_rel_comments = ' # '.join(sorted_com) if sorted_com else ''\n",
    "    return sorted_score_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\", \"test\"]\n",
    "datasets = {\n",
    "    \"users\": {\n",
    "        \"path_input_format\":path_raw_data + 'r3_{target}_{split}_users.csv', \n",
    "        \"path_output_format\":path_processed_data + 'r3_{target}_{split}_users_scored_Timeline.csv', \n",
    "        \"path_output_format_L\":path_processed_data + 'r3_{target}_{split}_users_scored_Timeline' + f'_L={L}_.csv', \n",
    "        \"text_col\": \"Timeline\"\n",
    "    },\n",
    "    \"tmt\":{\n",
    "        \"path_input_format\":path_raw_data + '{split}_r3_{target}_top_mentioned_timelines.csv',\n",
    "        \"path_output_format\":path_processed_data + '{split}_r3_{target}_top_mentioned_timelines_scored_Texts.csv',\n",
    "        \"path_output_format_L\":path_processed_data + '{split}_r3_{target}_top_mentioned_timelines_scored_Texts'+ f'_L={L}_.csv',\n",
    "        \"text_col\": \"Texts\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "# Running dataset:users | target:ig\n",
      "########################################\n",
      "# train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1043/1796 [03:17<02:58,  4.21it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "######################\n",
    "# Process\n",
    "######################\n",
    "\n",
    "dict_final = {}\n",
    "for dataset_name, config in datasets.items():\n",
    "\n",
    "    \n",
    "\n",
    "    dict_dfs = {}\n",
    "    for target, terms_list in target_terms_dict.items():\n",
    "        \n",
    "        print(f\"\"\"########################################\n",
    "# Running dataset:{dataset_name} | target:{target}\n",
    "########################################\"\"\")\n",
    "        \n",
    "        dict_splits = {}\n",
    "\n",
    "        for split in [\n",
    "            \"train\", \n",
    "            \"test\"\n",
    "            ]:\n",
    "            \n",
    "            print(f'# {split}')\n",
    "        \n",
    "            path_data = config['path_input_format'].format(split = split, target = target)\n",
    "            path_output_L = config['path_output_format_L'].format(split = split, target = target)\n",
    "            path_output_normal = config['path_output_format'].format(split = split, target = target)\n",
    "            \n",
    "            # read data\n",
    "            data = pd.read_csv(\n",
    "                path_data,\n",
    "                sep = ';', \n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "                    \n",
    "            \n",
    "            new_col = f'comments_and_scores_{config['text_col']}'\n",
    "            \n",
    "            data[new_col] = data[config['text_col']].progress_apply(lambda x: find_relevant_comments(x, terms_list))\n",
    "            \n",
    "            data.to_csv(path_output_normal,index = False)\n",
    "            \n",
    "            \n",
    "            data_L = data.copy()\n",
    "            data_L[config['text_col'] + f\"_L={L}\"] = data_L[new_col].progress_apply(lambda x: \" # \".join([comment for score, comment in x[-L:]])) \n",
    "            \n",
    "            data_L.to_csv(path_output_L, index = False)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dict_splits.update({split:data})\n",
    "            \n",
    "            \n",
    "        dict_dfs.update({target:dict_splits})\n",
    "        \n",
    "    dict_final.update({dataset_name:dict_dfs})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
