{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from models.ClassificationPipeline import ClassificationPipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/semcovici/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test, y_pred):\n",
    "    '''Source: https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format'''\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    df_classification_report = pd.DataFrame(report).transpose()\n",
    "    df_classification_report = df_classification_report.sort_values(by=['f1-score'], ascending=False)\n",
    "    return df_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_data = '../data/raw/'\n",
    "path_processed_data = '../data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'ig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read data\n",
    "data = pd.read_csv(path_processed_data + f'train_r3_{corpus}_filtered.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2_ig_1</td>\n",
       "      <td>against</td>\n",
       "      <td>PQP ESSE DORAMA É MUITO FOADA(Sassy GoGo(Cheer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>Golaço!!!!!!!!! # Manda geral do time principa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2_ig_7</td>\n",
       "      <td>against</td>\n",
       "      <td>@gabycunha86 Amanhã vou aí, deixa pra terça # ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2_ig_8</td>\n",
       "      <td>for</td>\n",
       "      <td>3.4- O Centro de Coordenação da Operação está ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2_ig_10</td>\n",
       "      <td>for</td>\n",
       "      <td>Me arrependi de excluir meu outro tt, agora ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User_ID Polarity                                              Texts\n",
       "0   r2_ig_1  against  PQP ESSE DORAMA É MUITO FOADA(Sassy GoGo(Cheer...\n",
       "1   r2_ig_4      for  Golaço!!!!!!!!! # Manda geral do time principa...\n",
       "2   r2_ig_7  against  @gabycunha86 Amanhã vou aí, deixa pra terça # ...\n",
       "3   r2_ig_8      for  3.4- O Centro de Coordenação da Operação está ...\n",
       "4  r2_ig_10      for  Me arrependi de excluir meu outro tt, agora ti..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1522, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Texts']\n",
    "y = data.Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    PQP ESSE DORAMA É MUITO FOADA(Sassy GoGo(Cheer...\n",
       "1    Golaço!!!!!!!!! # Manda geral do time principa...\n",
       "2    @gabycunha86 Amanhã vou aí, deixa pra terça # ...\n",
       "3    3.4- O Centro de Coordenação da Operação está ...\n",
       "4    Me arrependi de excluir meu outro tt, agora ti...\n",
       "Name: Texts, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1790    0\n",
       "1792    0\n",
       "1793    0\n",
       "1794    0\n",
       "1795    0\n",
       "Name: Polarity, Length: 1522, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode label\n",
    "y_encoded = y.map({'against': 0, 'for': 1})\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sem oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect = TfidfVectorizer(\n",
    "            stop_words = stopwords.words('portuguese'),\n",
    "            lowercase = True,\n",
    "            ngram_range = (1,3),\n",
    "            max_features=30000\n",
    "            \n",
    "            )\n",
    "sampling = None\n",
    "scaling = MaxAbsScaler()\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "            random_state = 42,\n",
    "            verbosity = 3,\n",
    "            # device = 'cuda',\n",
    "            # tree_method = 'hist'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m clf_pipe \u001b[38;5;241m=\u001b[39m ClassificationPipeline(\n\u001b[1;32m      2\u001b[0m     vectorizer\u001b[38;5;241m=\u001b[39mtext_vect,\n\u001b[1;32m      3\u001b[0m     sampling \u001b[38;5;241m=\u001b[39m sampling,\n\u001b[1;32m      4\u001b[0m     scaling \u001b[38;5;241m=\u001b[39mscaling, \n\u001b[1;32m      5\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m classifier\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mclf_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m y_pred, y_pred_proba \u001b[38;5;241m=\u001b[39m clf_pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     12\u001b[0m df_classification_report \u001b[38;5;241m=\u001b[39m get_classification_report(y_test, y_pred)\n",
      "File \u001b[0;32m~/pesquisa/stance-prediction-UstanceBR/notebooks/../src/models/ClassificationPipeline.py:27\u001b[0m, in \u001b[0;36mClassificationPipeline.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe_trained\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/imblearn/pipeline.py:322\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03mFit all the transforms/samplers one after the other and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    This estimator.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 322\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/imblearn/pipeline.py:248\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    246\u001b[0m     cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    258\u001b[0m     X, y, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_resample_one_cached(\n\u001b[1;32m    259\u001b[0m         cloned_transformer,\n\u001b[1;32m    260\u001b[0m         X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    265\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/imblearn/pipeline.py:1097\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1097\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1100\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1101\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2137\u001b[0m )\n\u001b[0;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_pipe = ClassificationPipeline(\n",
    "    vectorizer=text_vect,\n",
    "    sampling = sampling,\n",
    "    scaling =scaling, \n",
    "    estimator= classifier\n",
    ")\n",
    "\n",
    "clf_pipe.train(X_train, y_train)\n",
    "\n",
    "y_pred, y_pred_proba = clf_pipe.predict(X_test)\n",
    "\n",
    "df_classification_report = get_classification_report(y_test, y_pred)\n",
    "\n",
    "df_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### com oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect = TfidfVectorizer(\n",
    "            stop_words = stopwords.words('portuguese'),\n",
    "            lowercase = True,\n",
    "            ngram_range = (1,3),\n",
    "            max_features=30000\n",
    "            \n",
    "            )\n",
    "sampling = RandomOverSampler(random_state=random_seed)\n",
    "scaling = MaxAbsScaler()\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "            random_state = 42,\n",
    "            verbosity = 3,\n",
    "            # device = 'cuda',\n",
    "            # tree_method = 'hist'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 4) Processing vectorizer, total= 3.2min\n",
      "[Pipeline] .......... (step 2 of 4) Processing sampling, total=   0.0s\n",
      "[Pipeline] ........... (step 3 of 4) Processing scaling, total=   0.2s\n",
      "[19:10:05] ======== Monitor (0): HostSketchContainer ========\n",
      "[19:10:05] AllReduce: 0.04035s, 1 calls @ 40350us\n",
      "\n",
      "[19:10:05] MakeCuts: 0.066544s, 1 calls @ 66544us\n",
      "\n",
      "[19:10:05] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[19:11:12] ======== Monitor (0): Learner ========\n",
      "[19:11:12] Configure: 0.000331s, 1 calls @ 331us\n",
      "\n",
      "[19:11:12] EvalOneIter: 0.001106s, 100 calls @ 1106us\n",
      "\n",
      "[19:11:12] GetGradient: 0.121384s, 100 calls @ 121384us\n",
      "\n",
      "[19:11:12] PredictRaw: 0.000148s, 100 calls @ 148us\n",
      "\n",
      "[19:11:12] UpdateOneIter: 67.1137s, 100 calls @ 67113677us\n",
      "\n",
      "[19:11:12] ======== Monitor (0): GBTree ========\n",
      "[19:11:12] BoostNewTrees: 66.9907s, 100 calls @ 66990683us\n",
      "\n",
      "[19:11:12] CommitModel: 8.8e-05s, 100 calls @ 88us\n",
      "\n",
      "[19:11:12] ======== Monitor (0): HistUpdater ========\n",
      "[19:11:12] BuildHistogram: 27.8145s, 483 calls @ 27814487us\n",
      "\n",
      "[19:11:12] EvaluateSplits: 27.8319s, 583 calls @ 27831948us\n",
      "\n",
      "[19:11:12] InitData: 0.137498s, 100 calls @ 137498us\n",
      "\n",
      "[19:11:12] InitRoot: 12.0574s, 100 calls @ 12057381us\n",
      "\n",
      "[19:11:12] LeafPartition: 3.9e-05s, 100 calls @ 39us\n",
      "\n",
      "[19:11:12] UpdatePosition: 1.54945s, 556 calls @ 1549454us\n",
      "\n",
      "[19:11:12] UpdatePredictionCache: 0.155958s, 100 calls @ 155958us\n",
      "\n",
      "[19:11:12] UpdateTree: 66.8333s, 100 calls @ 66833333us\n",
      "\n",
      "[Pipeline] ......... (step 4 of 4) Processing estimator, total= 1.1min\n",
      "[19:11:21] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674556</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.680597</td>\n",
       "      <td>166.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.64918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.648621</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.648838</td>\n",
       "      <td>305.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.646102</td>\n",
       "      <td>0.645532</td>\n",
       "      <td>0.645753</td>\n",
       "      <td>305.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.610909</td>\n",
       "      <td>139.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "0              0.674556  0.686747  0.680597  166.00000\n",
       "accuracy       0.649180  0.649180  0.649180    0.64918\n",
       "weighted avg   0.648621  0.649180  0.648838  305.00000\n",
       "macro avg      0.646102  0.645532  0.645753  305.00000\n",
       "1              0.617647  0.604317  0.610909  139.00000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipe = ClassificationPipeline(\n",
    "    vectorizer=text_vect,\n",
    "    sampling = sampling,\n",
    "    scaling =scaling, \n",
    "    estimator= classifier\n",
    ")\n",
    "\n",
    "clf_pipe.train(X_train, y_train)\n",
    "\n",
    "y_pred, y_pred_proba = clf_pipe.predict(X_test)\n",
    "\n",
    "df_classification_report = get_classification_report(y_test, y_pred)\n",
    "\n",
    "df_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com feature indicando a quantidade de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Texts</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2_ig_1</td>\n",
       "      <td>against</td>\n",
       "      <td>PQP ESSE DORAMA É MUITO FOADA(Sassy GoGo(Cheer...</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>Golaço!!!!!!!!! # Manda geral do time principa...</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2_ig_7</td>\n",
       "      <td>against</td>\n",
       "      <td>@gabycunha86 Amanhã vou aí, deixa pra terça # ...</td>\n",
       "      <td>956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2_ig_8</td>\n",
       "      <td>for</td>\n",
       "      <td>3.4- O Centro de Coordenação da Operação está ...</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2_ig_10</td>\n",
       "      <td>for</td>\n",
       "      <td>Me arrependi de excluir meu outro tt, agora ti...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>r2_ig_2395</td>\n",
       "      <td>against</td>\n",
       "      <td>ontem a rafaela me abandonou e eu fui pro omeg...</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>r2_ig_2398</td>\n",
       "      <td>against</td>\n",
       "      <td>Em todos os 0 estados dos EUA a partir de hoje...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>r2_ig_2399</td>\n",
       "      <td>against</td>\n",
       "      <td>isso daqui so eu jogando prime 0 # se rolar ví...</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>r2_ig_2400</td>\n",
       "      <td>against</td>\n",
       "      <td>@amndwz_ nao acredito! # @amndwz_ MUDANÇA # @a...</td>\n",
       "      <td>2129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>r2_ig_2402</td>\n",
       "      <td>against</td>\n",
       "      <td>@bmazzeo Vamos mudar isso, Bruno! Estamos espe...</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1522 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User_ID Polarity                                              Texts  \\\n",
       "0        r2_ig_1  against  PQP ESSE DORAMA É MUITO FOADA(Sassy GoGo(Cheer...   \n",
       "1        r2_ig_4      for  Golaço!!!!!!!!! # Manda geral do time principa...   \n",
       "2        r2_ig_7  against  @gabycunha86 Amanhã vou aí, deixa pra terça # ...   \n",
       "3        r2_ig_8      for  3.4- O Centro de Coordenação da Operação está ...   \n",
       "4       r2_ig_10      for  Me arrependi de excluir meu outro tt, agora ti...   \n",
       "...          ...      ...                                                ...   \n",
       "1790  r2_ig_2395  against  ontem a rafaela me abandonou e eu fui pro omeg...   \n",
       "1792  r2_ig_2398  against  Em todos os 0 estados dos EUA a partir de hoje...   \n",
       "1793  r2_ig_2399  against  isso daqui so eu jogando prime 0 # se rolar ví...   \n",
       "1794  r2_ig_2400  against  @amndwz_ nao acredito! # @amndwz_ MUDANÇA # @a...   \n",
       "1795  r2_ig_2402  against  @bmazzeo Vamos mudar isso, Bruno! Estamos espe...   \n",
       "\n",
       "      n_comments  \n",
       "0            878  \n",
       "1            533  \n",
       "2            956  \n",
       "3           1153  \n",
       "4             91  \n",
       "...          ...  \n",
       "1790         312  \n",
       "1792          94  \n",
       "1793        2579  \n",
       "1794        2129  \n",
       "1795         431  \n",
       "\n",
       "[1522 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_comments'] = data.Texts.apply(lambda x: len(x.split(' # ')))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Texts','n_comments']]\n",
    "y = data.Polarity\n",
    "y_encoded = y.map({'against': 0, 'for': 1})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect = TfidfVectorizer(\n",
    "            stop_words = stopwords.words('portuguese'),\n",
    "            lowercase = True,\n",
    "            ngram_range = (1,3),\n",
    "            max_features=30000\n",
    "            \n",
    "            )\n",
    "sampling = RandomOverSampler(random_state=random_seed)\n",
    "scaling = MaxAbsScaler()\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "            random_state = 42,\n",
    "            verbosity = 3,\n",
    "            # device = 'cuda',\n",
    "            # tree_method = 'hist'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Texts', text_vect , 'Texts'),\n",
    "        ('n_comments', 'passthrough',['n_comments'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 4) Processing vectorizer, total= 3.2min\n",
      "[Pipeline] .......... (step 2 of 4) Processing sampling, total=   0.0s\n",
      "[Pipeline] ........... (step 3 of 4) Processing scaling, total=   0.2s\n",
      "[19:19:57] ======== Monitor (0): HostSketchContainer ========\n",
      "[19:19:57] AllReduce: 0.05322s, 1 calls @ 53220us\n",
      "\n",
      "[19:19:57] MakeCuts: 0.092076s, 1 calls @ 92076us\n",
      "\n",
      "[19:19:57] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n",
      "[19:21:07] ======== Monitor (0): Learner ========\n",
      "[19:21:07] Configure: 0.000706s, 1 calls @ 706us\n",
      "\n",
      "[19:21:07] EvalOneIter: 0.001146s, 100 calls @ 1146us\n",
      "\n",
      "[19:21:07] GetGradient: 0.092159s, 100 calls @ 92159us\n",
      "\n",
      "[19:21:07] PredictRaw: 0.000135s, 100 calls @ 135us\n",
      "\n",
      "[19:21:07] UpdateOneIter: 69.4355s, 100 calls @ 69435540us\n",
      "\n",
      "[19:21:07] ======== Monitor (0): GBTree ========\n",
      "[19:21:07] BoostNewTrees: 69.341s, 100 calls @ 69340959us\n",
      "\n",
      "[19:21:07] CommitModel: 8.2e-05s, 100 calls @ 82us\n",
      "\n",
      "[19:21:07] ======== Monitor (0): HistUpdater ========\n",
      "[19:21:07] BuildHistogram: 27.6426s, 483 calls @ 27642623us\n",
      "\n",
      "[19:21:07] EvaluateSplits: 30.1437s, 583 calls @ 30143700us\n",
      "\n",
      "[19:21:07] InitData: 0.078161s, 100 calls @ 78161us\n",
      "\n",
      "[19:21:07] InitRoot: 12.371s, 100 calls @ 12370955us\n",
      "\n",
      "[19:21:07] LeafPartition: 4.8e-05s, 100 calls @ 48us\n",
      "\n",
      "[19:21:07] UpdatePosition: 2.32696s, 556 calls @ 2326963us\n",
      "\n",
      "[19:21:07] UpdatePredictionCache: 0.10046s, 100 calls @ 100460us\n",
      "\n",
      "[19:21:07] UpdateTree: 69.2391s, 100 calls @ 69239114us\n",
      "\n",
      "[Pipeline] ......... (step 4 of 4) Processing estimator, total= 1.2min\n",
      "[19:21:17] DEBUG: /workspace/src/gbm/gbtree.cc:130: Using tree method: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674556</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.680597</td>\n",
       "      <td>166.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.64918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.648621</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.648838</td>\n",
       "      <td>305.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.646102</td>\n",
       "      <td>0.645532</td>\n",
       "      <td>0.645753</td>\n",
       "      <td>305.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.610909</td>\n",
       "      <td>139.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score    support\n",
       "0              0.674556  0.686747  0.680597  166.00000\n",
       "accuracy       0.649180  0.649180  0.649180    0.64918\n",
       "weighted avg   0.648621  0.649180  0.648838  305.00000\n",
       "macro avg      0.646102  0.645532  0.645753  305.00000\n",
       "1              0.617647  0.604317  0.610909  139.00000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipe = ClassificationPipeline(\n",
    "    vectorizer=preprocessor,\n",
    "    sampling = sampling,\n",
    "    scaling =scaling, \n",
    "    estimator= classifier\n",
    ")\n",
    "\n",
    "clf_pipe.train(X_train, y_train)\n",
    "\n",
    "y_pred, y_pred_proba = clf_pipe.predict(X_test)\n",
    "\n",
    "df_classification_report = get_classification_report(y_test, y_pred)\n",
    "\n",
    "df_classification_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
