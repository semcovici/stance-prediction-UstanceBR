{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/semcovici/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "raw_data_path = '../data/raw/'\n",
    "processed_data_path = '../data/processed/'\n",
    "results_cr_path = '../reports/classification_reports/'\n",
    "test_results_path = '../reports/test_results/'\n",
    "reports_path = '../reports/'\n",
    "\n",
    "target_list = ['ig','bo', 'cl', 'co', 'gl', 'lu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:21<00:00,  3.62s/it]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "\n",
    "for target in tqdm(target_list):\n",
    "    \n",
    "    # read data\n",
    "    data_temp_train = pd.read_csv(\n",
    "        raw_data_path + f'train_r3_{target}_top_mentioned_timelines.csv', \n",
    "        sep = ';', \n",
    "        encoding='utf-8-sig'\n",
    "        )\n",
    "    data_temp_test = pd.read_csv(\n",
    "        raw_data_path + f'test_r3_{target}_top_mentioned_timelines.csv', \n",
    "        sep = ';', \n",
    "        encoding='utf-8-sig'\n",
    "        )\n",
    "    \n",
    "    data_temp_train['target'] = target\n",
    "    data_temp_test['target'] = target\n",
    "    \n",
    "    data_temp_train['split'] = \"train\"\n",
    "    data_temp_test['split'] = \"test\"\n",
    "    \n",
    "    data_list.append(data_temp_train)\n",
    "    data_list.append(data_temp_test)\n",
    "    \n",
    "data_tmt = pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:27<00:00,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "\n",
    "for target in tqdm(target_list):\n",
    "    \n",
    "    # read data\n",
    "    data_temp_train = pd.read_csv(\n",
    "        raw_data_path + f'r3_{target}_train_users.csv', \n",
    "        sep = ';', \n",
    "        encoding='utf-8-sig'\n",
    "    )\n",
    "    data_temp_test = pd.read_csv(\n",
    "        raw_data_path + f'r3_{target}_test_users.csv', \n",
    "        sep = ';', \n",
    "        encoding='utf-8-sig'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    data_temp_train['target'] = target\n",
    "    data_temp_test['target'] = target\n",
    "    \n",
    "    data_temp_train['split'] = \"train\"\n",
    "    data_temp_test['split'] = \"test\"\n",
    "    \n",
    "    data_list.append(data_temp_train)\n",
    "    data_list.append(data_temp_test)\n",
    "    \n",
    "data_users = pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    0.749822\n",
       "test     0.250178\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_users.split.value_counts()/len(data_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    0.749822\n",
       "test     0.250178\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_tmt.split.value_counts()/len(data_tmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_comments(\n",
    "    data,\n",
    "    Texts_col = 'Texts',\n",
    "    sep = ' # '\n",
    "):\n",
    "    \n",
    "    # Separates texts into individual lines\n",
    "    df_sep_comments = data.assign(Texts=data[Texts_col].str.split(sep)).explode(Texts_col)\n",
    "    \n",
    "    df_sep_comments.rename({\"Texts\":Texts_col},axis = 1)\n",
    "\n",
    "    # Reindex the resulting DataFrame\n",
    "    df_sep_comments.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    df_sep_comments.ffill(inplace = True)\n",
    "    \n",
    "    return df_sep_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cp = {\n",
    "    'cl':'Hydrox.',\n",
    "    'lu':'Lula',\n",
    "    'co':'Sinovac',\n",
    "    'ig':'Church',\n",
    "    'gl':'Globo TV',\n",
    "    'bo':'Bolsonaro',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Timeline</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tweet_Seq</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2_ig_1</td>\n",
       "      <td>@ posso nem comer meu pãozin de queijo em paz ...</td>\n",
       "      <td>tenho pra mim que grande parte senão todas as ...</td>\n",
       "      <td>against</td>\n",
       "      <td>2953</td>\n",
       "      <td>ig</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>Fim de jogo ++ uma vitoria do meu Vascão # Hoj...</td>\n",
       "      <td>Cidade de Deus Alicate: quer saber vou entrar ...</td>\n",
       "      <td>for</td>\n",
       "      <td>4792</td>\n",
       "      <td>ig</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2_ig_7</td>\n",
       "      <td>Meu chefe é todo aleatório, do nada chega com ...</td>\n",
       "      <td>Acordei já sendo removida do grupo da igreja</td>\n",
       "      <td>against</td>\n",
       "      <td>248</td>\n",
       "      <td>ig</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2_ig_8</td>\n",
       "      <td>veja a receita FILÉ COM MOLHO DE MOSTARDA # Di...</td>\n",
       "      <td>I liked a @ video culto infantil na igreja Ass...</td>\n",
       "      <td>for</td>\n",
       "      <td>45</td>\n",
       "      <td>ig</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2_ig_10</td>\n",
       "      <td>Oq tem de gente boa, tem de irritante # Não te...</td>\n",
       "      <td>Essa turma da igreja sao tão amorzinho smp con...</td>\n",
       "      <td>for</td>\n",
       "      <td>3809</td>\n",
       "      <td>ig</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>r2_lu_1086</td>\n",
       "      <td>Gostei de um vídeo @ … com Sweet Carol | The N...</td>\n",
       "      <td>Eu deveria me espelhar no Lula e ler 55 página...</td>\n",
       "      <td>for</td>\n",
       "      <td>381</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>r2_lu_1090</td>\n",
       "      <td>Show de bola! Que venham outros … # Essa renda...</td>\n",
       "      <td>Pqp quanta merda em um Tweet só! Pare de mistu...</td>\n",
       "      <td>against</td>\n",
       "      <td>899</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>r2_lu_1091</td>\n",
       "      <td>FOOOOOOOOOOOOOOGOOOOOOOOOOOOOOO!!!!!!! # \"200 ...</td>\n",
       "      <td>nem a Venezuela respeita mais o Brasil sem o L...</td>\n",
       "      <td>for</td>\n",
       "      <td>294</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>r2_lu_1093</td>\n",
       "      <td>@ quem prejudica a imagem do Brasil não é o po...</td>\n",
       "      <td>Lula tem uma visão de mundo muito diversa de F...</td>\n",
       "      <td>for</td>\n",
       "      <td>2021</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>r2_lu_1094</td>\n",
       "      <td>O Globo: Osama bin Ladem está morto, diz CNN #...</td>\n",
       "      <td>gente, o Lula não pode ir de avião não, olha o...</td>\n",
       "      <td>for</td>\n",
       "      <td>2551</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_ID                                           Timeline  \\\n",
       "0       r2_ig_1  @ posso nem comer meu pãozin de queijo em paz ...   \n",
       "1       r2_ig_4  Fim de jogo ++ uma vitoria do meu Vascão # Hoj...   \n",
       "2       r2_ig_7  Meu chefe é todo aleatório, do nada chega com ...   \n",
       "3       r2_ig_8  veja a receita FILÉ COM MOLHO DE MOSTARDA # Di...   \n",
       "4      r2_ig_10  Oq tem de gente boa, tem de irritante # Não te...   \n",
       "..          ...                                                ...   \n",
       "267  r2_lu_1086  Gostei de um vídeo @ … com Sweet Carol | The N...   \n",
       "268  r2_lu_1090  Show de bola! Que venham outros … # Essa renda...   \n",
       "269  r2_lu_1091  FOOOOOOOOOOOOOOGOOOOOOOOOOOOOOO!!!!!!! # \"200 ...   \n",
       "270  r2_lu_1093  @ quem prejudica a imagem do Brasil não é o po...   \n",
       "271  r2_lu_1094  O Globo: Osama bin Ladem está morto, diz CNN #...   \n",
       "\n",
       "                                                Stance Polarity  Tweet_Seq  \\\n",
       "0    tenho pra mim que grande parte senão todas as ...  against       2953   \n",
       "1    Cidade de Deus Alicate: quer saber vou entrar ...      for       4792   \n",
       "2         Acordei já sendo removida do grupo da igreja  against        248   \n",
       "3    I liked a @ video culto infantil na igreja Ass...      for         45   \n",
       "4    Essa turma da igreja sao tão amorzinho smp con...      for       3809   \n",
       "..                                                 ...      ...        ...   \n",
       "267  Eu deveria me espelhar no Lula e ler 55 página...      for        381   \n",
       "268  Pqp quanta merda em um Tweet só! Pare de mistu...  against        899   \n",
       "269  nem a Venezuela respeita mais o Brasil sem o L...      for        294   \n",
       "270  Lula tem uma visão de mundo muito diversa de F...      for       2021   \n",
       "271  gente, o Lula não pode ir de avião não, olha o...      for       2551   \n",
       "\n",
       "    target  split  \n",
       "0       ig  train  \n",
       "1       ig  train  \n",
       "2       ig  train  \n",
       "3       ig  train  \n",
       "4       ig  train  \n",
       "..     ...    ...  \n",
       "267     lu   test  \n",
       "268     lu   test  \n",
       "269     lu   test  \n",
       "270     lu   test  \n",
       "271     lu   test  \n",
       "\n",
       "[11264 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_config = {\n",
    "    'top_mentioned_timelines':{\n",
    "        'data': data_tmt,\n",
    "        'columns': {\n",
    "            'Texts': 1\n",
    "        }\n",
    "    },\n",
    "    'users':{\n",
    "        'data': data_users,\n",
    "        'columns':{\n",
    "            'Timeline': 1,\n",
    "            'Stance': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dict_results = {}\n",
    "\n",
    "# for name, config in dict_config.items():\n",
    "    \n",
    "    \n",
    "#     data = config[\"data\"]\n",
    "    \n",
    "#     for column, multiple_comments in config[\"columns\"].items():\n",
    "        \n",
    "        \n",
    "#         df_anl = pd.DataFrame({\n",
    "#             \"Target\": [],\n",
    "#             \"Against\": [],\n",
    "#             \"For\": [],\n",
    "#             \"All\": [],\n",
    "#             \"Words\": [],\n",
    "#             \"Comments/User\": [],\n",
    "#             \"W/Tweet\": []\n",
    "#         })\n",
    "        \n",
    "#         for i, target in enumerate(target_list):\n",
    "            \n",
    "#             print(f'##### Start Running {target} ({i+1} of {len(target_list)}) #####')\n",
    "            \n",
    "#             df_target = data[data.target == target]\n",
    "            \n",
    "#             counts_target = df_target.Polarity.value_counts()\n",
    "            \n",
    "#             n_against = counts_target['against']\n",
    "#             n_for = counts_target['for']\n",
    "            \n",
    "#             if multiple_comments:\n",
    "#                 # separate comments and drop the duplicates (the comments that appears in more the one user)\n",
    "#                 df_sep_comments = separate_comments(df_target, Texts_col = column).drop_duplicates(subset=[column])\n",
    "            \n",
    "#                 # create column with tokens\n",
    "#                 df_sep_comments['tokens'] = df_sep_comments[column].progress_apply(lambda x: word_tokenize(x, language='portuguese'))\n",
    "#                 # create column with count of tokens\n",
    "#                 df_sep_comments['tokens_count'] = df_sep_comments.tokens.progress_apply(len)\n",
    "                \n",
    "#                 gpby_userid = df_sep_comments.groupby('User_ID')\n",
    "#                 count_users = len(df_sep_comments.User_ID.unique())\n",
    "            \n",
    "#                 new_row = {\n",
    "#                     \"Target\": target,\n",
    "#                     \"Against\": n_against,\n",
    "#                     \"For\": n_for,\n",
    "#                     \"All\": n_against + n_for,\n",
    "#                     \"Words\": df_sep_comments.tokens_count.sum(),\n",
    "#                     \"Comments/User\": gpby_userid.size().sum() / count_users,\n",
    "#                     \"W/Tweet\": df_sep_comments.tokens_count.sum()/len(df_sep_comments)\n",
    "#                 }\n",
    "                \n",
    "#                 df_anl.loc[len(df_anl)] = new_row\n",
    "                \n",
    "#             else:\n",
    "                \n",
    "#                 # create column with tokens\n",
    "#                 df_target['tokens'] = df_target[column].progress_apply(lambda x: word_tokenize(x, language='portuguese'))\n",
    "#                 # create column with count of tokens\n",
    "#                 df_target['tokens_count'] = df_target.tokens.progress_apply(len)\n",
    "                \n",
    "#                 gpby_userid = df_target.groupby('User_ID')\n",
    "#                 count_users = len(df_target.User_ID.unique())\n",
    "            \n",
    "#                 new_row = {\n",
    "#                     \"Target\": target,\n",
    "#                     \"Against\": n_against,\n",
    "#                     \"For\": n_for,\n",
    "#                     \"All\": n_against + n_for,\n",
    "#                     \"Words\": df_target.tokens_count.sum(),\n",
    "#                     \"Comments/User\": \"nsa\",\n",
    "#                     \"W/Tweet\": df_target.tokens_count.sum()/len(df_target)\n",
    "#                 }\n",
    "                \n",
    "#                 df_anl.loc[len(df_anl)] = new_row\n",
    "                \n",
    "            \n",
    "#             print(f'##### End Running {target} ({i+1} of {len(target_list)}) #####')\n",
    "            \n",
    "#         df_anl.Target = df_anl.Target.map(dict_cp)\n",
    "\n",
    "        \n",
    "#         counts_target = data.Polarity.value_counts()\n",
    "\n",
    "#         n_against = counts_target['against']\n",
    "#         n_for = counts_target['for']\n",
    "\n",
    "\n",
    "#         if multiple_comments:\n",
    "#             comment_user = df_anl[\"Comments/User\"].sum()/len(df_anl)\n",
    "#         else:\n",
    "#             comment_user = \"nsa\"\n",
    "            \n",
    "\n",
    "#         new_row = {\n",
    "#             \"Target\": \"Overall\",\n",
    "#             \"Against\": n_against,\n",
    "#             \"For\": n_for,\n",
    "#             \"All\": n_against + n_for,\n",
    "#             \"Words\": df_anl.Words.sum(),\n",
    "#             \"Comments/User\": comment_user,\n",
    "#             \"W/Tweet\": df_anl[\"W/Tweet\"].sum()/len(df_anl)\n",
    "#         }\n",
    "\n",
    "#         df_anl.loc[len(df_anl)] = new_row\n",
    "#         df_anl = df_anl.round(2)\n",
    "        \n",
    "        \n",
    "#         dict_results.update({f\"{name}_{column}\":df_anl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_row\n\u001b[1;32m     55\u001b[0m dict_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, config \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdict_config\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     58\u001b[0m     data \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column, multiple_comments \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_config' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def separate_comments(df, Texts_col):\n",
    "    # Assuming 'User_ID' is the column with user IDs\n",
    "    df_grouped = df.groupby('User_ID')[Texts_col].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    return df_grouped\n",
    "\n",
    "def process_target(target, data, column, multiple_comments):\n",
    "    print(f'##### Start Running {target} #####')\n",
    "    \n",
    "    df_target = data[data.target == target]\n",
    "    counts_target = df_target.Polarity.value_counts()\n",
    "    n_against = counts_target.get('against', 0)\n",
    "    n_for = counts_target.get('for', 0)\n",
    "    \n",
    "    if multiple_comments:\n",
    "        df_sep_comments = separate_comments(df_target, column).drop_duplicates(subset=[column])\n",
    "        df_sep_comments['tokens'] = df_sep_comments[column].progress_apply(lambda x: word_tokenize(x, language='portuguese'))\n",
    "        df_sep_comments['tokens_count'] = df_sep_comments.tokens.progress_apply(len)\n",
    "        gpby_userid = df_sep_comments.groupby('User_ID')\n",
    "        count_users = len(df_sep_comments.User_ID.unique())\n",
    "        \n",
    "        new_row = {\n",
    "            \"Target\": target,\n",
    "            \"Against\": n_against,\n",
    "            \"For\": n_for,\n",
    "            \"All\": n_against + n_for,\n",
    "            \"Words\": df_sep_comments.tokens_count.sum(),\n",
    "            \"Comments/User\": gpby_userid.size().sum() / count_users,\n",
    "            \"W/Tweet\": df_sep_comments.tokens_count.sum() / len(df_sep_comments)\n",
    "        }\n",
    "    else:\n",
    "        df_target['tokens'] = df_target[column].progress_apply(lambda x: word_tokenize(x, language='portuguese'))\n",
    "        df_target['tokens_count'] = df_target.tokens.progress_apply(len)\n",
    "        gpby_userid = df_target.groupby('User_ID')\n",
    "        count_users = len(df_target.User_ID.unique())\n",
    "        \n",
    "        new_row = {\n",
    "            \"Target\": target,\n",
    "            \"Against\": n_against,\n",
    "            \"For\": n_for,\n",
    "            \"All\": n_against + n_for,\n",
    "            \"Words\": df_target.tokens_count.sum(),\n",
    "            \"Comments/User\": \"nsa\",\n",
    "            \"W/Tweet\": df_target.tokens_count.sum() / len(df_target)\n",
    "        }\n",
    "        \n",
    "    print(f'##### End Running {target} #####')\n",
    "    return new_row\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "for name, config in dict_config.items():\n",
    "    data = config[\"data\"]\n",
    "    \n",
    "    for column, multiple_comments in config[\"columns\"].items():\n",
    "        df_anl = pd.DataFrame(columns=[\"Target\", \"Against\", \"For\", \"All\", \"Words\", \"Comments/User\", \"W/Tweet\"])\n",
    "        target_list = data['target'].unique()\n",
    "        \n",
    "        # Process each target sequentially, can be optimized further with parallel processing\n",
    "        results = [process_target(target, data, column, multiple_comments) for target in target_list]\n",
    "        \n",
    "        df_anl.loc[len(df_anl)] = results\n",
    "                \n",
    "        df_anl['Target'] = df_anl['Target'].map(dict_cp)\n",
    "        \n",
    "        counts_target = data.Polarity.value_counts()\n",
    "        n_against = counts_target.get('against', 0)\n",
    "        n_for = counts_target.get('for', 0)\n",
    "        \n",
    "        if multiple_comments:\n",
    "            comment_user = df_anl[\"Comments/User\"].sum() / len(df_anl)\n",
    "        else:\n",
    "            comment_user = \"nsa\"\n",
    "        \n",
    "        overall_row = {\n",
    "            \"Target\": \"Overall\",\n",
    "            \"Against\": n_against,\n",
    "            \"For\": n_for,\n",
    "            \"All\": n_against + n_for,\n",
    "            \"Words\": df_anl.Words.sum(),\n",
    "            \"Comments/User\": comment_user,\n",
    "            \"W/Tweet\": df_anl[\"W/Tweet\"].sum() / len(df_anl)\n",
    "        }\n",
    "        \n",
    "        df_anl.loc[len(df_anl)] = overall_row\n",
    "        df_anl = df_anl.round(2)\n",
    "        \n",
    "        dict_results[f\"{name}_{column}\"] = df_anl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Timeline</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tweet_Seq</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2_lu_1</td>\n",
       "      <td>Foda-se # Hey @ palmeiras é grande ? # @ você ...</td>\n",
       "      <td>Lula tem copa e Bolsonaro não</td>\n",
       "      <td>for</td>\n",
       "      <td>736</td>\n",
       "      <td>lu</td>\n",
       "      <td>train</td>\n",
       "      <td>[Lula, tem, copa, e, Bolsonaro, não]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2_lu_2</td>\n",
       "      <td>@ posso nem comer meu pãozin de queijo em paz ...</td>\n",
       "      <td>nao nao nao nao nao nao eu acabei de ver uma p...</td>\n",
       "      <td>for</td>\n",
       "      <td>2898</td>\n",
       "      <td>lu</td>\n",
       "      <td>train</td>\n",
       "      <td>[nao, nao, nao, nao, nao, nao, eu, acabei, de,...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2_lu_3</td>\n",
       "      <td>Sai da frente !! # No papel, qual é a melhor d...</td>\n",
       "      <td>Reclama de Bolsonaro mas vota em Lula 🤦‍♂️</td>\n",
       "      <td>against</td>\n",
       "      <td>5736</td>\n",
       "      <td>lu</td>\n",
       "      <td>train</td>\n",
       "      <td>[Reclama, de, Bolsonaro, mas, vota, em, Lula, ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2_lu_5</td>\n",
       "      <td>KKKKJJJJJKKKKKKKKKK a gente na aula de filosof...</td>\n",
       "      <td>Enviei minha poesia que fala de política p mar...</td>\n",
       "      <td>for</td>\n",
       "      <td>402</td>\n",
       "      <td>lu</td>\n",
       "      <td>train</td>\n",
       "      <td>[Enviei, minha, poesia, que, fala, de, polític...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2_lu_9</td>\n",
       "      <td>O Finazzi jogava assim também, a bola tinha qu...</td>\n",
       "      <td>Por isso q o país está entregue a esses asnos ...</td>\n",
       "      <td>for</td>\n",
       "      <td>3415</td>\n",
       "      <td>lu</td>\n",
       "      <td>train</td>\n",
       "      <td>[Por, isso, q, o, país, está, entregue, a, ess...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>r2_lu_1086</td>\n",
       "      <td>Gostei de um vídeo @ … com Sweet Carol | The N...</td>\n",
       "      <td>Eu deveria me espelhar no Lula e ler 55 página...</td>\n",
       "      <td>for</td>\n",
       "      <td>381</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "      <td>[Eu, deveria, me, espelhar, no, Lula, e, ler, ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>r2_lu_1090</td>\n",
       "      <td>Show de bola! Que venham outros … # Essa renda...</td>\n",
       "      <td>Pqp quanta merda em um Tweet só! Pare de mistu...</td>\n",
       "      <td>against</td>\n",
       "      <td>899</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "      <td>[Pqp, quanta, merda, em, um, Tweet, só, !, Par...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>r2_lu_1091</td>\n",
       "      <td>FOOOOOOOOOOOOOOGOOOOOOOOOOOOOOO!!!!!!! # \"200 ...</td>\n",
       "      <td>nem a Venezuela respeita mais o Brasil sem o L...</td>\n",
       "      <td>for</td>\n",
       "      <td>294</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "      <td>[nem, a, Venezuela, respeita, mais, o, Brasil,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>r2_lu_1093</td>\n",
       "      <td>@ quem prejudica a imagem do Brasil não é o po...</td>\n",
       "      <td>Lula tem uma visão de mundo muito diversa de F...</td>\n",
       "      <td>for</td>\n",
       "      <td>2021</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "      <td>[Lula, tem, uma, visão, de, mundo, muito, dive...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>r2_lu_1094</td>\n",
       "      <td>O Globo: Osama bin Ladem está morto, diz CNN #...</td>\n",
       "      <td>gente, o Lula não pode ir de avião não, olha o...</td>\n",
       "      <td>for</td>\n",
       "      <td>2551</td>\n",
       "      <td>lu</td>\n",
       "      <td>test</td>\n",
       "      <td>[gente, ,, o, Lula, não, pode, ir, de, avião, ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1088 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_ID                                           Timeline  \\\n",
       "0       r2_lu_1  Foda-se # Hey @ palmeiras é grande ? # @ você ...   \n",
       "1       r2_lu_2  @ posso nem comer meu pãozin de queijo em paz ...   \n",
       "2       r2_lu_3  Sai da frente !! # No papel, qual é a melhor d...   \n",
       "3       r2_lu_5  KKKKJJJJJKKKKKKKKKK a gente na aula de filosof...   \n",
       "4       r2_lu_9  O Finazzi jogava assim também, a bola tinha qu...   \n",
       "..          ...                                                ...   \n",
       "267  r2_lu_1086  Gostei de um vídeo @ … com Sweet Carol | The N...   \n",
       "268  r2_lu_1090  Show de bola! Que venham outros … # Essa renda...   \n",
       "269  r2_lu_1091  FOOOOOOOOOOOOOOGOOOOOOOOOOOOOOO!!!!!!! # \"200 ...   \n",
       "270  r2_lu_1093  @ quem prejudica a imagem do Brasil não é o po...   \n",
       "271  r2_lu_1094  O Globo: Osama bin Ladem está morto, diz CNN #...   \n",
       "\n",
       "                                                Stance Polarity  Tweet_Seq  \\\n",
       "0                        Lula tem copa e Bolsonaro não      for        736   \n",
       "1    nao nao nao nao nao nao eu acabei de ver uma p...      for       2898   \n",
       "2           Reclama de Bolsonaro mas vota em Lula 🤦‍♂️  against       5736   \n",
       "3    Enviei minha poesia que fala de política p mar...      for        402   \n",
       "4    Por isso q o país está entregue a esses asnos ...      for       3415   \n",
       "..                                                 ...      ...        ...   \n",
       "267  Eu deveria me espelhar no Lula e ler 55 página...      for        381   \n",
       "268  Pqp quanta merda em um Tweet só! Pare de mistu...  against        899   \n",
       "269  nem a Venezuela respeita mais o Brasil sem o L...      for        294   \n",
       "270  Lula tem uma visão de mundo muito diversa de F...      for       2021   \n",
       "271  gente, o Lula não pode ir de avião não, olha o...      for       2551   \n",
       "\n",
       "    target  split                                             tokens  \\\n",
       "0       lu  train               [Lula, tem, copa, e, Bolsonaro, não]   \n",
       "1       lu  train  [nao, nao, nao, nao, nao, nao, eu, acabei, de,...   \n",
       "2       lu  train  [Reclama, de, Bolsonaro, mas, vota, em, Lula, ...   \n",
       "3       lu  train  [Enviei, minha, poesia, que, fala, de, polític...   \n",
       "4       lu  train  [Por, isso, q, o, país, está, entregue, a, ess...   \n",
       "..     ...    ...                                                ...   \n",
       "267     lu   test  [Eu, deveria, me, espelhar, no, Lula, e, ler, ...   \n",
       "268     lu   test  [Pqp, quanta, merda, em, um, Tweet, só, !, Par...   \n",
       "269     lu   test  [nem, a, Venezuela, respeita, mais, o, Brasil,...   \n",
       "270     lu   test  [Lula, tem, uma, visão, de, mundo, muito, dive...   \n",
       "271     lu   test  [gente, ,, o, Lula, não, pode, ir, de, avião, ...   \n",
       "\n",
       "     tokens_count  \n",
       "0               6  \n",
       "1              49  \n",
       "2               8  \n",
       "3              33  \n",
       "4              56  \n",
       "..            ...  \n",
       "267            14  \n",
       "268            28  \n",
       "269            19  \n",
       "270            46  \n",
       "271            30  \n",
       "\n",
       "[1088 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polarity\n",
       "against    5811\n",
       "for        5453\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5453"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Against</th>\n",
       "      <th>For</th>\n",
       "      <th>All</th>\n",
       "      <th>Words</th>\n",
       "      <th>Comments/User</th>\n",
       "      <th>W/Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Church</td>\n",
       "      <td>1354</td>\n",
       "      <td>1041</td>\n",
       "      <td>2395</td>\n",
       "      <td>59148</td>\n",
       "      <td>nsa</td>\n",
       "      <td>24.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolsonaro</td>\n",
       "      <td>649</td>\n",
       "      <td>102</td>\n",
       "      <td>751</td>\n",
       "      <td>15901</td>\n",
       "      <td>nsa</td>\n",
       "      <td>21.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hydrox.</td>\n",
       "      <td>1154</td>\n",
       "      <td>1141</td>\n",
       "      <td>2295</td>\n",
       "      <td>68961</td>\n",
       "      <td>nsa</td>\n",
       "      <td>30.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sinovac</td>\n",
       "      <td>1416</td>\n",
       "      <td>1677</td>\n",
       "      <td>3093</td>\n",
       "      <td>92079</td>\n",
       "      <td>nsa</td>\n",
       "      <td>29.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Globo TV</td>\n",
       "      <td>668</td>\n",
       "      <td>974</td>\n",
       "      <td>1642</td>\n",
       "      <td>27484</td>\n",
       "      <td>nsa</td>\n",
       "      <td>16.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lula</td>\n",
       "      <td>570</td>\n",
       "      <td>518</td>\n",
       "      <td>1088</td>\n",
       "      <td>26567</td>\n",
       "      <td>nsa</td>\n",
       "      <td>24.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>5811</td>\n",
       "      <td>5453</td>\n",
       "      <td>11264</td>\n",
       "      <td>290140</td>\n",
       "      <td>nsa</td>\n",
       "      <td>24.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target  Against   For    All   Words Comments/User  W/Tweet\n",
       "0     Church     1354  1041   2395   59148           nsa    24.70\n",
       "1  Bolsonaro      649   102    751   15901           nsa    21.17\n",
       "2    Hydrox.     1154  1141   2295   68961           nsa    30.05\n",
       "3    Sinovac     1416  1677   3093   92079           nsa    29.77\n",
       "4   Globo TV      668   974   1642   27484           nsa    16.74\n",
       "5       Lula      570   518   1088   26567           nsa    24.42\n",
       "6    Overall     5811  5453  11264  290140           nsa    24.47"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5453"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_mentioned_timelines_Texts':       Target  Against   For    All      Words  Comments/User  W/Tweet\n",
       " 0     Church     1354  1041   2395   56364209        1874.95    14.98\n",
       " 1  Bolsonaro      649   102    751   14608017        1908.32    14.58\n",
       " 2    Hydrox.     1154  1141   2295   51770007        2172.46    22.17\n",
       " 3    Sinovac     1416  1677   3093   69421120        2147.33    20.58\n",
       " 4   Globo TV      668   974   1642   35004176        1932.34    15.15\n",
       " 5       Lula      570   518   1088   25731928        2030.16    18.24\n",
       " 6    Overall     5811  5453  11264  252899457        2010.93    17.61,\n",
       " 'users_Timeline':       Target  Against   For    All      Words  Comments/User   W/Tweet\n",
       " 0     Church     1354  1041   2395  140207645            1.0  58541.81\n",
       " 1  Bolsonaro      649   102    751   41224425            1.0  54892.71\n",
       " 2    Hydrox.     1154  1141   2295  108484625            1.0  47269.99\n",
       " 3    Sinovac     1416  1677   3093  137333979            1.0  44401.55\n",
       " 4   Globo TV      668   974   1642   93326479            1.0  56837.08\n",
       " 5       Lula      570   518   1088   67569624            1.0  62104.43\n",
       " 6    Overall     5811  5453  11264  588146777            1.0  54007.93,\n",
       " 'users_Stance':       Target  Against   For    All   Words Comments/User  W/Tweet\n",
       " 0     Church     1354  1041   2395   59148           nsa    24.70\n",
       " 1  Bolsonaro      649   102    751   15901           nsa    21.17\n",
       " 2    Hydrox.     1154  1141   2295   68961           nsa    30.05\n",
       " 3    Sinovac     1416  1677   3093   92079           nsa    29.77\n",
       " 4   Globo TV      668   974   1642   27484           nsa    16.74\n",
       " 5       Lula      570   518   1088   26567           nsa    24.42\n",
       " 6    Overall     5811  5453  11264  290140           nsa    24.47}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Target': 'Overall',\n",
       " 'Against': 5811,\n",
       " 'For': 5453,\n",
       " 'All': 11264,\n",
       " 'Words': 290140,\n",
       " 'Comments/User': 'nsa',\n",
       " 'W/Tweet': 24.474061390301298}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5811"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_target['against']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_mentioned_timelines_Texts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Against</th>\n",
       "      <th>For</th>\n",
       "      <th>All</th>\n",
       "      <th>Words</th>\n",
       "      <th>Comments/User</th>\n",
       "      <th>W/Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Church</td>\n",
       "      <td>1354</td>\n",
       "      <td>1041</td>\n",
       "      <td>2395</td>\n",
       "      <td>56364209</td>\n",
       "      <td>1874.95</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolsonaro</td>\n",
       "      <td>649</td>\n",
       "      <td>102</td>\n",
       "      <td>751</td>\n",
       "      <td>14608017</td>\n",
       "      <td>1908.32</td>\n",
       "      <td>14.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hydrox.</td>\n",
       "      <td>1154</td>\n",
       "      <td>1141</td>\n",
       "      <td>2295</td>\n",
       "      <td>51770007</td>\n",
       "      <td>2172.46</td>\n",
       "      <td>22.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sinovac</td>\n",
       "      <td>1416</td>\n",
       "      <td>1677</td>\n",
       "      <td>3093</td>\n",
       "      <td>69421120</td>\n",
       "      <td>2147.33</td>\n",
       "      <td>20.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Globo TV</td>\n",
       "      <td>668</td>\n",
       "      <td>974</td>\n",
       "      <td>1642</td>\n",
       "      <td>35004176</td>\n",
       "      <td>1932.34</td>\n",
       "      <td>15.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lula</td>\n",
       "      <td>570</td>\n",
       "      <td>518</td>\n",
       "      <td>1088</td>\n",
       "      <td>25731928</td>\n",
       "      <td>2030.16</td>\n",
       "      <td>18.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>5811</td>\n",
       "      <td>5453</td>\n",
       "      <td>11264</td>\n",
       "      <td>252899457</td>\n",
       "      <td>2010.93</td>\n",
       "      <td>17.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target  Against   For    All      Words  Comments/User  W/Tweet\n",
       "0     Church     1354  1041   2395   56364209        1874.95    14.98\n",
       "1  Bolsonaro      649   102    751   14608017        1908.32    14.58\n",
       "2    Hydrox.     1154  1141   2295   51770007        2172.46    22.17\n",
       "3    Sinovac     1416  1677   3093   69421120        2147.33    20.58\n",
       "4   Globo TV      668   974   1642   35004176        1932.34    15.15\n",
       "5       Lula      570   518   1088   25731928        2030.16    18.24\n",
       "6    Overall     5811  5453  11264  252899457        2010.93    17.61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_Timeline\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Against</th>\n",
       "      <th>For</th>\n",
       "      <th>All</th>\n",
       "      <th>Words</th>\n",
       "      <th>Comments/User</th>\n",
       "      <th>W/Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Church</td>\n",
       "      <td>1354</td>\n",
       "      <td>1041</td>\n",
       "      <td>2395</td>\n",
       "      <td>140207645</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58541.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolsonaro</td>\n",
       "      <td>649</td>\n",
       "      <td>102</td>\n",
       "      <td>751</td>\n",
       "      <td>41224425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54892.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hydrox.</td>\n",
       "      <td>1154</td>\n",
       "      <td>1141</td>\n",
       "      <td>2295</td>\n",
       "      <td>108484625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47269.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sinovac</td>\n",
       "      <td>1416</td>\n",
       "      <td>1677</td>\n",
       "      <td>3093</td>\n",
       "      <td>137333979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44401.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Globo TV</td>\n",
       "      <td>668</td>\n",
       "      <td>974</td>\n",
       "      <td>1642</td>\n",
       "      <td>93326479</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56837.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lula</td>\n",
       "      <td>570</td>\n",
       "      <td>518</td>\n",
       "      <td>1088</td>\n",
       "      <td>67569624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62104.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>5811</td>\n",
       "      <td>5453</td>\n",
       "      <td>11264</td>\n",
       "      <td>588146777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54007.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target  Against   For    All      Words  Comments/User   W/Tweet\n",
       "0     Church     1354  1041   2395  140207645            1.0  58541.81\n",
       "1  Bolsonaro      649   102    751   41224425            1.0  54892.71\n",
       "2    Hydrox.     1154  1141   2295  108484625            1.0  47269.99\n",
       "3    Sinovac     1416  1677   3093  137333979            1.0  44401.55\n",
       "4   Globo TV      668   974   1642   93326479            1.0  56837.08\n",
       "5       Lula      570   518   1088   67569624            1.0  62104.43\n",
       "6    Overall     5811  5453  11264  588146777            1.0  54007.93"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_Stance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Against</th>\n",
       "      <th>For</th>\n",
       "      <th>All</th>\n",
       "      <th>Words</th>\n",
       "      <th>Comments/User</th>\n",
       "      <th>W/Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Church</td>\n",
       "      <td>1354</td>\n",
       "      <td>1041</td>\n",
       "      <td>2395</td>\n",
       "      <td>59148</td>\n",
       "      <td>nsa</td>\n",
       "      <td>24.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolsonaro</td>\n",
       "      <td>649</td>\n",
       "      <td>102</td>\n",
       "      <td>751</td>\n",
       "      <td>15901</td>\n",
       "      <td>nsa</td>\n",
       "      <td>21.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hydrox.</td>\n",
       "      <td>1154</td>\n",
       "      <td>1141</td>\n",
       "      <td>2295</td>\n",
       "      <td>68961</td>\n",
       "      <td>nsa</td>\n",
       "      <td>30.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sinovac</td>\n",
       "      <td>1416</td>\n",
       "      <td>1677</td>\n",
       "      <td>3093</td>\n",
       "      <td>92079</td>\n",
       "      <td>nsa</td>\n",
       "      <td>29.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Globo TV</td>\n",
       "      <td>668</td>\n",
       "      <td>974</td>\n",
       "      <td>1642</td>\n",
       "      <td>27484</td>\n",
       "      <td>nsa</td>\n",
       "      <td>16.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lula</td>\n",
       "      <td>570</td>\n",
       "      <td>518</td>\n",
       "      <td>1088</td>\n",
       "      <td>26567</td>\n",
       "      <td>nsa</td>\n",
       "      <td>24.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>5811</td>\n",
       "      <td>5453</td>\n",
       "      <td>11264</td>\n",
       "      <td>290140</td>\n",
       "      <td>nsa</td>\n",
       "      <td>24.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target  Against   For    All   Words Comments/User  W/Tweet\n",
       "0     Church     1354  1041   2395   59148           nsa    24.70\n",
       "1  Bolsonaro      649   102    751   15901           nsa    21.17\n",
       "2    Hydrox.     1154  1141   2295   68961           nsa    30.05\n",
       "3    Sinovac     1416  1677   3093   92079           nsa    29.77\n",
       "4   Globo TV      668   974   1642   27484           nsa    16.74\n",
       "5       Lula      570   518   1088   26567           nsa    24.42\n",
       "6    Overall     5811  5453  11264  290140           nsa    24.47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, value in dict_results.items():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    display(value)\n",
    "    \n",
    "    value.to_csv(reports_path + f'describe/{key}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
