{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bo: 100%|██████████| 188/188 [14:40<00:00,  4.68s/it]\n",
      "bo:  28%|██▊       | 52/188 [03:14<08:14,  3.64s/it]"
     ]
    }
   ],
   "source": [
    "# !ollama pull llama3\n",
    "# !ollama pull llama3:70b\n",
    "\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, f1_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "from models.classification_methods import get_classification_report\n",
    "\n",
    "#############################\n",
    "# Definitions\n",
    "#############################\n",
    "random_seed = 42\n",
    "\n",
    "raw_data_path = 'data/raw/'\n",
    "processed_data_path = 'data/processed/'\n",
    "results_cr_path = 'reports/classification_reports/'\n",
    "test_results_path = 'reports/test_results/'\n",
    "reports_path = 'reports/'\n",
    "\n",
    "target_list = [\n",
    "    'ig',\n",
    "    'bo', \n",
    "    'cl', \n",
    "    'co', \n",
    "    'gl', \n",
    "    'lu'\n",
    "    ]\n",
    "\n",
    "estimator_name = 'llama3'\n",
    "#estimator_name = 'llama3:70b'\n",
    "\n",
    "dict_cp = {\n",
    "    'cl':'Hidroxicloroquina',\n",
    "    'lu':'Lula',\n",
    "    'co':'Sinovac',\n",
    "    'ig':'Church',\n",
    "    'gl':'Globo TV',\n",
    "    'bo':'Bolsonaro',\n",
    "}\n",
    "\n",
    "file_format_users = 'r3_{target}_test_users.csv'\n",
    "file_format_tmt = 'test_r3_{target}_top_mentioned_timelines.csv'\n",
    "\n",
    "dict_experiments = {\n",
    "    'Texts': {\n",
    "        \"text_col\": 'Texts',\n",
    "        \"prompts_to_test\": ['prompt2_Texts'],\n",
    "        \"is_multi_text\": True,\n",
    "        \"n_comments\": 30,\n",
    "        \"file_format\": file_format_tmt\n",
    "    },\n",
    "    'Timeline': {\n",
    "        \"text_col\": 'Timeline',\n",
    "        \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "        \"is_multi_text\": True,\n",
    "        \"n_comments\": 30,\n",
    "        \"file_format\": file_format_users\n",
    "    },\n",
    "    'Stance': {\n",
    "        \"text_col\": 'Stance',\n",
    "        \"prompts_to_test\": ['prompt2_Stance'],\n",
    "        \"is_multi_text\": False,\n",
    "        \"file_format\": file_format_users\n",
    "    }\n",
    "}\n",
    "\n",
    "#############################\n",
    "# Aux Functions\n",
    "#############################\n",
    "def get_response_from_llm(prompt):\n",
    "    response_full = ollama.generate(model=estimator_name, prompt = prompt)\n",
    "    return response_full\n",
    "\n",
    "\n",
    "def format_response(\n",
    "    response,\n",
    "    threshold = 0.5\n",
    "    ):\n",
    "    \n",
    "    message = response['response']\n",
    "        \n",
    "    try:\n",
    "        # string dict to dict\n",
    "        response = eval(message)\n",
    "        \n",
    "        if response < threshold:\n",
    "            y_pred = 0\n",
    "        else:\n",
    "            y_pred = 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        y_pred = None \n",
    "        \n",
    "    return message, y_pred\n",
    "\n",
    "def get_prompt(prompt_name):\n",
    "    \n",
    "    with open(f'src/models/config/prompts/{prompt_name}.txt', 'r') as file:\n",
    "        \n",
    "        prompt_template = file.read()\n",
    "        \n",
    "    return prompt_template \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "# Process\n",
    "#############################\n",
    "\n",
    "\n",
    "for exp_name, config in dict_experiments.items():\n",
    "    \n",
    "    # get configs of experiments\n",
    "    text_col = config['text_col']\n",
    "    prompts_to_test = config['prompts_to_test']\n",
    "    is_multi_text = config['is_multi_text']\n",
    "    file_format = config['file_format']\n",
    "    \n",
    "    data_list = []\n",
    "    for target in target_list:\n",
    "        \n",
    "        # read data\n",
    "        data_aux = pd.read_csv(raw_data_path + file_format.format(target=target), sep = ';', encoding='utf-8-sig')\n",
    "        \n",
    "        data_aux['target'] = target\n",
    "        \n",
    "        data_list.append(data_aux)\n",
    "    \n",
    "    # create final test df\n",
    "    test_df = pd.concat(data_list)\n",
    "\n",
    "    # test all prompts\n",
    "    for prompt_name in prompts_to_test:\n",
    "        \n",
    "        # get prompt template from file\n",
    "        prompt_template = get_prompt(prompt_name)\n",
    "\n",
    "        dict_responses = {}\n",
    "\n",
    "        list_results = [] \n",
    "\n",
    "        list_df_responses = [] \n",
    "\n",
    "        for target in target_list:\n",
    "\n",
    "            df_responses = pd.DataFrame({\n",
    "                \"idx\":[],\n",
    "                \"text\":[],\n",
    "                \"target\":[],\n",
    "                \"y_test\":[],\n",
    "                \"y_pred\":[],\n",
    "                \"justification\":[],\n",
    "                \"complete_response\": []\n",
    "            })\n",
    "            \n",
    "            data = test_df[test_df['target'] == target]    \n",
    "                \n",
    "            for idx, row in tqdm(data.iterrows(), total = len(data), desc = target):\n",
    "                \n",
    "                text = row[text_col]\n",
    "                target_id = target\n",
    "                target = dict_cp.get(row['target'])\n",
    "                polarity = row[\"Polarity\"]\n",
    "                polarity = 1 if polarity == 'for' else 0\n",
    "                \n",
    "                if not is_multi_text:\n",
    "                \n",
    "                    prompt_formated = prompt_template.format(\n",
    "                    target = target,\n",
    "                    text = text)\n",
    "                    \n",
    "                else: \n",
    "                    \n",
    "                    n_comments = config['n_comments']\n",
    "                    \n",
    "                    # create list with comments and get the firt n comments\n",
    "                    comments = text.split(' # ')\n",
    "                    comments_filtered =  comments[:n_comments]\n",
    "                    \n",
    "                    texts = ''\n",
    "                    for c in comments_filtered:\n",
    "                        \n",
    "                        texts += '<comment>\\n'\n",
    "                        texts += c\n",
    "                        texts += '\\n</comment>'\n",
    "                                        \n",
    "                    prompt_formated = prompt_template.format(\n",
    "                    target = target,\n",
    "                    text = texts)\n",
    "                    \n",
    "                \n",
    "                response_full = get_response_from_llm(prompt_formated)\n",
    "                \n",
    "                message, y_pred = format_response(response_full)\n",
    "\n",
    "                new_row = {\n",
    "                \"idx\": idx,\n",
    "                \"text\":text,\n",
    "                \"target\":target,\n",
    "                \"y_test\": polarity,\n",
    "                \"y_pred\":y_pred,\n",
    "                \"complete_response\": message\n",
    "                \n",
    "                }\n",
    "                \n",
    "                df_responses.loc[len(df_responses)] = new_row\n",
    "                \n",
    "            df_responses['target'] = target\n",
    "            \n",
    "            list_df_responses.append(df_responses)\n",
    "            \n",
    "        df_results_final = pd.concat(list_df_responses)   \n",
    "\n",
    "        df_results_final.to_csv(f'{reports_path}test_results/{estimator_name}_{exp_name}_{prompt_name}_classification_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.43      0.59       162\n",
      "           1       0.17      0.73      0.28        26\n",
      "\n",
      "    accuracy                           0.47       188\n",
      "   macro avg       0.54      0.58      0.43       188\n",
      "weighted avg       0.81      0.47      0.54       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_results_final.y_test,df_results_final.y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
