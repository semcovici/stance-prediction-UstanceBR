{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification_methods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_classification_report,create_test_results_df\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlambdas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m int_to_label, label_to_int\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Definitions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#############################\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "# !ollama pull llama3\n",
    "# !ollama pull llama3:70b\n",
    "\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "tqdm.pandas()\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, f1_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "from models.classification_methods import get_classification_report,create_test_results_df\n",
    "from data.lambdas import int_to_label, label_to_int\n",
    "\n",
    "#############################\n",
    "# Definitions\n",
    "#############################\n",
    "random_seed = 42\n",
    "\n",
    "raw_data_path = 'data/raw/'\n",
    "processed_data_path = 'data/processed/'\n",
    "results_cr_path = 'reports/classification_reports/'\n",
    "test_results_path = 'reports/test_results/'\n",
    "reports_path = 'reports/'\n",
    "\n",
    "target_list = [\n",
    "    'ig',\n",
    "    'bo', \n",
    "    'cl', \n",
    "    'co', \n",
    "    'gl', \n",
    "    'lu'\n",
    "    ]\n",
    "\n",
    "estimator_name = 'llama3'\n",
    "#estimator_name = 'llama3:70b'\n",
    "\n",
    "dict_cp = {\n",
    "    'cl':'Hidroxicloroquina',\n",
    "    'lu':'Lula',\n",
    "    'co':'Sinovac',\n",
    "    'ig':'Church',\n",
    "    'gl':'Globo TV',\n",
    "    'bo':'Bolsonaro',\n",
    "}\n",
    "\n",
    "file_format_users = raw_data_path +'r3_{target}_test_users.csv'\n",
    "file_format_users_filtered = processed_data_path + 'r3_{target}_test_users_scored_Timeline.csv' \n",
    "file_format_tmt = raw_data_path +'test_r3_{target}_top_mentioned_timelines.csv'\n",
    "file_format_tmt_filtered = processed_data_path + 'test_r3_{target}_top_mentioned_timelines_scored_Texts.csv'\n",
    "\n",
    "\n",
    "dict_experiments = {\n",
    "    \n",
    "    # 'filtered_Texts40': {\n",
    "    #     \"text_col\": 'Texts',\n",
    "    #     \"prompts_to_test\": ['prompt2_Texts'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 40,\n",
    "    #     \"file_format\": file_format_tmt_filtered\n",
    "    # },\n",
    "    # 'filteredTimeline40': {\n",
    "    #     \"text_col\": 'Timeline',\n",
    "    #     \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 40,\n",
    "    #     \"file_format\": file_format_users_filtered\n",
    "    # },    \n",
    "\n",
    "    \n",
    "    # 'filtered_Texts30': {\n",
    "    #     \"text_col\": 'Texts',\n",
    "    #     \"prompts_to_test\": ['prompt2_Texts'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 30,\n",
    "    #     \"file_format\": file_format_tmt_filtered\n",
    "    # },\n",
    "    # 'filteredTimeline30': {\n",
    "    #     \"text_col\": 'Timeline',\n",
    "    #     \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 30,\n",
    "    #     \"file_format\": file_format_users_filtered\n",
    "    # },\n",
    "    \n",
    "    # 'filtered_Texts20': {\n",
    "    #     \"text_col\": 'Texts',\n",
    "    #     \"prompts_to_test\": ['prompt2_Texts'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 20,\n",
    "    #     \"file_format\": file_format_tmt_filtered\n",
    "    # },\n",
    "    # 'filteredTimeline20': {\n",
    "    #     \"text_col\": 'Timeline',\n",
    "    #     \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 20,\n",
    "    #     \"file_format\": file_format_users_filtered\n",
    "    # },\n",
    "    # 'filtered_Texts15': {\n",
    "    #     \"text_col\": 'Texts',\n",
    "    #     \"prompts_to_test\": ['prompt2_Texts'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 15,\n",
    "    #     \"file_format\": file_format_tmt_filtered\n",
    "    # },\n",
    "    # 'filteredTimeline15': {\n",
    "    #     \"text_col\": 'Timeline',\n",
    "    #     \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 15,\n",
    "    #     \"file_format\": file_format_users_filtered\n",
    "    # },\n",
    "    \n",
    "    # 'filtered_Texts5': {\n",
    "    #     \"text_col\": 'Texts',\n",
    "    #     \"prompts_to_test\": ['prompt2_Texts'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 5,\n",
    "    #     \"file_format\": file_format_tmt_filtered\n",
    "    # },\n",
    "    # 'filteredTimeline5': {\n",
    "    #     \"text_col\": 'Timeline',\n",
    "    #     \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 5,\n",
    "    #     \"file_format\": file_format_users_filtered\n",
    "    # },\n",
    "    # 'filtered_Texts10': {\n",
    "    #     \"text_col\": 'Texts',\n",
    "    #     \"prompts_to_test\": ['prompt2_Texts'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 10,\n",
    "    #     \"file_format\": file_format_tmt_filtered\n",
    "    # },\n",
    "    # 'filteredTimeline10': {\n",
    "    #     \"text_col\": 'Timeline',\n",
    "    #     \"prompts_to_test\": ['prompt2_Timeline'],\n",
    "    #     \"is_multi_text\": True,\n",
    "    #     \"n_comments\": 10,\n",
    "    #     \"file_format\": file_format_users_filtered\n",
    "    # },\n",
    "    'Stance': {\n",
    "        \"text_col\": 'Stance',\n",
    "        \"prompts_to_test\": ['prompt2_Stance'],\n",
    "        \"is_multi_text\": False,\n",
    "        \"file_format\": file_format_users\n",
    "    }\n",
    "}\n",
    "\n",
    "#############################\n",
    "# Aux Functions\n",
    "#############################\n",
    "def get_response_from_llm(prompt):\n",
    "    response_full = ollama.generate(\n",
    "        model=estimator_name, \n",
    "        prompt = prompt, \n",
    "        options = {\n",
    "            'seed': random_seed,\n",
    "            'temperature': 0,\n",
    "            \"num_gpu\": 1,\n",
    "            \"main_gpu\": 0,\n",
    "            }\n",
    "        )\n",
    "    return response_full\n",
    "\n",
    "\n",
    "def format_response(\n",
    "    response,\n",
    "    target,\n",
    "    threshold = 0.5\n",
    "    ):\n",
    "    \n",
    "    message = response['response']\n",
    "        \n",
    "    try:\n",
    "        # string dict to dict\n",
    "        response = eval(message)\n",
    "        \n",
    "        if response < threshold:\n",
    "            y_pred = 0\n",
    "        else:\n",
    "            y_pred = 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        \n",
    "        # when get an error, the prediction is the most common in train\n",
    "        y_pred = dict_majority[target]\n",
    "        \n",
    "    return message, response, y_pred\n",
    "\n",
    "def get_prompt(prompt_name):\n",
    "    \n",
    "    with open(f'src/models/config/prompts/{prompt_name}.txt', 'r') as file:\n",
    "        \n",
    "        prompt_template = file.read()\n",
    "        \n",
    "    return prompt_template \n",
    "\n",
    "\n",
    "check_if_already_exists = False\n",
    "\n",
    "print(\"Creating dict with majority polarity in train\")\n",
    "dict_majority = {}\n",
    "for target in tqdm(target_list):\n",
    "    \n",
    "    df = pd.read_csv( f\"data/raw/r3_{target}_train_users.csv\", sep = ';', encoding='utf-8-sig')\n",
    "    majority = label_to_int(df.Polarity.value_counts().idxmax())\n",
    "    dict_majority.update({target:majority})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "# Process\n",
    "#############################\n",
    "for exp_name, config in dict_experiments.items():\n",
    "    \n",
    "    print(f\"\"\"####################################  \n",
    "# Running {exp_name}\n",
    "#####################################\"\"\")\n",
    "    \n",
    "    \n",
    "    # get configs of experiments\n",
    "    text_col = config['text_col']\n",
    "    prompts_to_test = config['prompts_to_test']\n",
    "    is_multi_text = config['is_multi_text']\n",
    "    file_format = config['file_format']\n",
    "    \n",
    "    \n",
    "    data_list = []\n",
    "    for target in target_list:\n",
    "        \n",
    "        # read data\n",
    "        data_aux = pd.read_csv( file_format.format(target=target), sep = ';', encoding='utf-8-sig')\n",
    "        \n",
    "        data_aux['target'] = target\n",
    "        \n",
    "        data_list.append(data_aux)\n",
    "    \n",
    "    # create final test df\n",
    "    test_df = pd.concat(data_list)\n",
    "\n",
    "    # test all prompts\n",
    "    for prompt_name in prompts_to_test:\n",
    "        \n",
    "        output_file = f'{reports_path}test_results/{estimator_name}_{exp_name}_{prompt_name}_test_results.csv'\n",
    "        \n",
    "        \n",
    "        if os.path.isfile(output_file) and check_if_already_exists:\n",
    "            print('# experiment already done')\n",
    "            continue\n",
    "        \n",
    "        # get prompt template from file\n",
    "        prompt_template = get_prompt(prompt_name)\n",
    "\n",
    "        dict_responses = {}\n",
    "\n",
    "        list_results = [] \n",
    "\n",
    "        list_df_responses = [] \n",
    "\n",
    "        for target in target_list:\n",
    "            \n",
    "            data = test_df[test_df['target'] == target].head(5)    \n",
    "                                    \n",
    "            # if is multi_text, filter only the best n comments\n",
    "            if is_multi_text:\n",
    "                n_comments = config['n_comments'] \n",
    "                data[f'comments_and_scores_{text_col}'] = data[f'comments_and_scores_{text_col}'].progress_apply(lambda x: literal_eval(x))\n",
    "                data[text_col] = data[f'comments_and_scores_{text_col}'].progress_apply(\n",
    "                    lambda x: \" # \".join([comment for score, comment in x[-n_comments:]])\n",
    "                    ) \n",
    "            \n",
    "            list_polarity_pred = []\n",
    "            pred_proba_0 = []\n",
    "            pred_proba_1 = []\n",
    "            list_message = []\n",
    "            for idx, row in tqdm(data.iterrows(), total = len(data), desc = target):\n",
    "                \n",
    "                text = row[text_col]\n",
    "                target_id = target\n",
    "                target = dict_cp.get(row['target'])\n",
    "                polarity = row[\"Polarity\"]\n",
    "                polarity = 1 if polarity == 'for' else 0\n",
    "                \n",
    "                \n",
    "                if not is_multi_text:\n",
    "                \n",
    "                    prompt_formated = prompt_template.format(\n",
    "                    target = target,\n",
    "                    text = text)\n",
    "                    \n",
    "                else: \n",
    "                    \n",
    "                    # create list with comments and get the firt n comments\n",
    "                    try:\n",
    "                        comments = text.split(' # ')\n",
    "                        comments_filtered =  comments[:n_comments]\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        \n",
    "                        comments = []\n",
    "                    \n",
    "                    texts = ''\n",
    "                    for c in comments_filtered:\n",
    "                        \n",
    "                        texts += '<comment>\\n'\n",
    "                        texts += c\n",
    "                        texts += '\\n</comment>'\n",
    "                                        \n",
    "                    prompt_formated = prompt_template.format(\n",
    "                    target = target,\n",
    "                    text = texts)\n",
    "                    \n",
    "                \n",
    "                response_full = get_response_from_llm(prompt_formated)\n",
    "                \n",
    "                message, response, y_pred = format_response(response_full, target)\n",
    "                \n",
    "                # create probas\n",
    "                if y_pred is not None:\n",
    "                    if y_pred == 0:\n",
    "                        proba_0 = response\n",
    "                        proba_1 = 1 - response\n",
    "                    elif y_pred == 1:\n",
    "                        proba_1 = response\n",
    "                        proba_0 = 1 - response\n",
    "                    else:\n",
    "                        raise Exception(\"Erro\")\n",
    "                    \n",
    "                else:\n",
    "                    proba_1 = -1\n",
    "                    proba_0 = -1\n",
    "                    \n",
    "                list_message.append(message)\n",
    "                list_polarity_pred.append(y_pred)\n",
    "                \n",
    "                pred_proba_0.append(float(proba_0))\n",
    "                pred_proba_1.append(float(proba_1))\n",
    "                \n",
    "            \n",
    "            y_test = data['Polarity'].tolist()\n",
    "            \n",
    "            \n",
    "            # format test and pred\n",
    "            y_test_formated = [int_to_label(test) for test in y_test]\n",
    "            y_pred_formated = [int_to_label(pred) for pred in list_polarity_pred]\n",
    "            \n",
    "            # create df with results\n",
    "            df_test_results = create_test_results_df(\n",
    "                y_test_formated, \n",
    "                y_pred_formated, \n",
    "                pred_proba_0, \n",
    "                pred_proba_1\n",
    "                )\n",
    "            \n",
    "            df_test_results['message'] = list_message \n",
    "            \n",
    "            df_test_results.to_csv(f'{reports_path}test_results/{estimator_name}_{target}_{exp_name}_{prompt_name}_test_results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
