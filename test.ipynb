{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "# Running: Stance \n",
      "###########################################\n",
      "######## target: ig\n",
      "# experiment already done\n",
      "######## target: bo\n",
      "# experiment already done\n",
      "######## target: cl\n",
      "# experiment already done\n",
      "######## target: co\n",
      "# experiment already done\n",
      "######## target: gl\n",
      "# experiment already done\n",
      "######## target: lu\n",
      "# experiment already done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "tqdm.pandas()\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "random.seed(0)\n",
    "from belt_nlp.bert_with_pooling import BertClassifierWithPooling\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"src/\")\n",
    "from models.classification_methods import create_test_results_df\n",
    "from data.lambdas import int_to_label, label_to_int\n",
    "\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = 'data/raw/'\n",
    "processed_data_path = 'data/processed/'\n",
    "reports_path = 'reports/'\n",
    "file_format_tmt = processed_data_path + \"{split}_r3_{target}_top_mentioned_timelines_processed.csv\"\n",
    "file_format_users = processed_data_path + 'r3_{target}_{split}_users_processed.csv' \n",
    "\n",
    "# Target list\n",
    "target_list = [\n",
    "    'ig',\n",
    "    'bo', \n",
    "    'cl', \n",
    "    'co', \n",
    "    'gl', \n",
    "    'lu'\n",
    "]\n",
    "\n",
    "# Definir o modelo pré-treinado\n",
    "model_name = \"pablocosta/bertabaporu-base-uncased\"\n",
    "\n",
    "# Função para tokenizar os dados\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Verificar se a GPU está disponível e definir o dispositivo\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "dict_exps = {\n",
    "    \"Stance\": {\n",
    "        'path_dataset': file_format_users,\n",
    "        \"text_col\": \"Stance\",\n",
    "    },\n",
    "    \"Timeline\": {\n",
    "        'path_dataset': file_format_users,\n",
    "        \"text_col\": \"Timeline\",\n",
    "    },\n",
    "    \"Texts\": {\n",
    "        'path_dataset': file_format_tmt,\n",
    "        \"text_col\": \"Texts\",\n",
    "    },\n",
    "}\n",
    "\n",
    "check_if_already_exists = True\n",
    "\n",
    "\n",
    "for exp_name, config in dict_exps.items():\n",
    "    \n",
    "    \n",
    "    print(f\"\"\"###########################################\n",
    "# Running: {exp_name} \n",
    "###########################################\"\"\")\n",
    "    \n",
    "    \n",
    "    text_col = config['text_col']\n",
    "    path_dataset = config['path_dataset']\n",
    "    \n",
    "    # Processar cada target\n",
    "    for target in target_list:\n",
    "        \n",
    "        print(f\"\"\"######## target: {target}\"\"\")\n",
    "        estimator_name = \"bert_classifier_\" + model_name.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "        test_results_path = f\"{reports_path}test_results/{estimator_name}_{target}_{text_col}_test_results.csv\"\n",
    "\n",
    "        \n",
    "        if os.path.isfile(test_results_path) and check_if_already_exists:\n",
    "            print('# experiment already done')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Ler e dividir os dados\n",
    "        train_val = pd.read_csv(\n",
    "            path_dataset.format(target=target, split=\"train\"), \n",
    "            sep=';', \n",
    "            encoding='utf-8-sig'\n",
    "        ).reset_index()[[text_col, 'Polarity']].rename(columns={text_col: 'text', 'Polarity': 'label'})\n",
    "        \n",
    "        train_val.label = train_val.label.apply(lambda x: label_to_int(x))\n",
    "\n",
    "        # check if label is binary\n",
    "        if len(train_val.label.unique()) != 2:\n",
    "            raise Exception(\"There is an error in train_val label transformation: expected to be binary\")\n",
    "        \n",
    "        train, val = train_test_split(train_val, test_size=0.15, random_state=42)\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        val.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        test = pd.read_csv(\n",
    "            path_dataset.format(target=target, split=\"test\"), \n",
    "            sep=';', \n",
    "            encoding='utf-8-sig'\n",
    "        ).reset_index()[[text_col, 'Polarity']].rename(columns={text_col: 'text', 'Polarity': 'label'})\n",
    "        \n",
    "        test.label = test.label.apply(lambda x: label_to_int(x))\n",
    "\n",
    "        # check if label is binary\n",
    "        if len(test.label.unique()) != 2:\n",
    "            raise Exception(\"There is an error in test label transformation: expected to be binary\")\n",
    "        \n",
    "        # Criar datasets do Hugging Face\n",
    "        train_dataset = Dataset.from_pandas(train)\n",
    "        val_dataset = Dataset.from_pandas(val)\n",
    "        test_dataset = Dataset.from_pandas(test)\n",
    "        \n",
    "        # Tokenizar os datasets\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        tokenized_datasets = DatasetDict({\n",
    "            'train': train_dataset.map(tokenize_function, batched=True),\n",
    "            'val': val_dataset.map(tokenize_function, batched=True),\n",
    "            'test': test_dataset.map(tokenize_function, batched=True)\n",
    "        })\n",
    "\n",
    "        # Carregar o modelo e mover para o dispositivo (GPU se disponível)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "        # Definir os argumentos de treinamento\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results/{target}',\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'./logs/{target}',\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        # Definir o Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets['train'],\n",
    "            eval_dataset=tokenized_datasets['val'],\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        # Treinar o modelo\n",
    "        trainer.train()\n",
    "\n",
    "        # Avaliar o modelo no conjunto de teste\n",
    "        eval_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "        print(f\"Evaluation results for {target}: {eval_results}\")\n",
    "\n",
    "        # Predição no conjunto de teste\n",
    "        test_predictions = trainer.predict(test_dataset=tokenized_datasets['test'])\n",
    "        test_pred_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "        # get logits\n",
    "        test_pred_logits = test_predictions.predictions\n",
    "        # transform logits in \"probabilities\"\n",
    "        test_pred_probs = softmax(torch.tensor(test_pred_logits), dim=-1).numpy()\n",
    "\n",
    "        # create list of proba of each class\n",
    "        pred_proba_0 = [float(probas[0]) for probas in test_pred_probs]\n",
    "        pred_proba_1 = [float(probas[1]) for probas in test_pred_probs]\n",
    "\n",
    "        # create list of test and prediction\n",
    "        y_test = test['label'].tolist()\n",
    "        y_pred = test_pred_labels.tolist()\n",
    "\n",
    "        # format test and pred\n",
    "        y_test_formated = [int_to_label(test) for test in y_test]\n",
    "        y_pred_formated = [int_to_label(pred) for pred in y_pred]\n",
    "\n",
    "        # create df with results\n",
    "        df_test_results = create_test_results_df(y_test_formated, y_pred_formated, pred_proba_0, pred_proba_1)\n",
    "        \n",
    "        df_test_results.to_csv(test_results_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
