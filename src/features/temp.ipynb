{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### START PROCESS - 2024-04-18 13:32:04.852449 #####\n",
      "\n",
      "Configuration:\n",
      "- model_name: neuralmind/bert-base-portuguese-cased\n",
      "- target: ig\n",
      "- split: train\n",
      "- dataset: top_mentioned_timelines\n",
      "            \n",
      "##### START USER r2_ig_1 (1 of 1522) - 2024-04-18 13:32:08.802693 #####\n",
      "r2_ig_1 already exists\n",
      "##### START USER r2_ig_4 (2 of 1522) - 2024-04-18 13:32:09.200863 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:32:10.345974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:04<00:00, 109.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:32:15.159415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 72.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_4 (2 of 1522) - 2024-04-18 13:32:16.719023 #####\n",
      "##### START USER r2_ig_7 (3 of 1522) - 2024-04-18 13:32:17.067877 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:32:18.341352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:08<00:00, 107.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:32:27.152887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 93.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_7 (3 of 1522) - 2024-04-18 13:32:28.676873 #####\n",
      "##### START USER r2_ig_8 (4 of 1522) - 2024-04-18 13:32:29.009347 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:32:30.315736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1149/1149 [00:14<00:00, 78.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:32:44.884563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 326.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_8 (4 of 1522) - 2024-04-18 13:32:46.338315 #####\n",
      "##### START USER r2_ig_10 (5 of 1522) - 2024-04-18 13:32:46.664601 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:32:47.808190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:00<00:00, 103.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:32:48.690281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 518.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_10 (5 of 1522) - 2024-04-18 13:32:50.118402 #####\n",
      "##### START USER r2_ig_11 (6 of 1522) - 2024-04-18 13:32:50.450579 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:32:51.706923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2082/2082 [00:21<00:00, 98.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:33:12.791684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 255.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_11 (6 of 1522) - 2024-04-18 13:33:14.275481 #####\n",
      "##### START USER r2_ig_12 (7 of 1522) - 2024-04-18 13:33:14.622767 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:33:16.262398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2427/2427 [00:24<00:00, 100.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:33:40.428161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 249.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_12 (7 of 1522) - 2024-04-18 13:33:41.901172 #####\n",
      "##### START USER r2_ig_13 (8 of 1522) - 2024-04-18 13:33:42.234968 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:33:43.751747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2490/2490 [00:22<00:00, 108.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:34:06.639086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 254.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_13 (8 of 1522) - 2024-04-18 13:34:08.078308 #####\n",
      "##### START USER r2_ig_14 (9 of 1522) - 2024-04-18 13:34:08.412123 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:34:09.739799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 116.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:34:14.037443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 389.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_14 (9 of 1522) - 2024-04-18 13:34:15.429275 #####\n",
      "##### START USER r2_ig_15 (10 of 1522) - 2024-04-18 13:34:15.762753 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:34:16.840877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2079/2079 [00:18<00:00, 111.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:34:35.466122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 254.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_15 (10 of 1522) - 2024-04-18 13:34:36.890862 #####\n",
      "##### START USER r2_ig_16 (11 of 1522) - 2024-04-18 13:34:37.214848 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:34:38.323333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:00<00:00, 89.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:34:39.052237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 540.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_16 (11 of 1522) - 2024-04-18 13:34:40.444110 #####\n",
      "##### START USER r2_ig_17 (12 of 1522) - 2024-04-18 13:34:40.778364 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:34:42.100156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:11<00:00, 109.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:34:53.498003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 323.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_17 (12 of 1522) - 2024-04-18 13:34:54.932112 #####\n",
      "##### START USER r2_ig_18 (13 of 1522) - 2024-04-18 13:34:55.256999 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:34:56.373963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:19<00:00, 103.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:35:15.682526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 288.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_18 (13 of 1522) - 2024-04-18 13:35:17.174425 #####\n",
      "##### START USER r2_ig_21 (14 of 1522) - 2024-04-18 13:35:17.508344 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:35:18.726868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2924/2924 [00:32<00:00, 88.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:35:51.668296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 215.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_21 (14 of 1522) - 2024-04-18 13:35:53.194207 #####\n",
      "##### START USER r2_ig_22 (15 of 1522) - 2024-04-18 13:35:53.527699 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:35:54.793067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1495/1495 [00:14<00:00, 106.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:36:08.801829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 327.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_22 (15 of 1522) - 2024-04-18 13:36:10.239470 #####\n",
      "##### START USER r2_ig_23 (16 of 1522) - 2024-04-18 13:36:10.569531 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:36:11.756253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2523/2523 [00:23<00:00, 105.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:36:35.627238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 235.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_23 (16 of 1522) - 2024-04-18 13:36:37.176696 #####\n",
      "##### START USER r2_ig_24 (17 of 1522) - 2024-04-18 13:36:37.507172 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:36:38.635247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2188/2188 [00:20<00:00, 105.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Embedding. Datetime: 2024-04-18 13:36:59.320424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:00<00:00, 254.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### END USER r2_ig_24 (17 of 1522) - 2024-04-18 13:37:00.789087 #####\n",
      "##### START USER r2_ig_25 (18 of 1522) - 2024-04-18 13:37:01.126098 #####\n",
      "Running neuralmind/bert-base-portuguese-cased. Datetime start: 2024-04-18 13:37:02.373935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 139/1275 [00:01<00:11, 96.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 104\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m data_user \u001b[38;5;241m=\u001b[39m data[data\u001b[38;5;241m.\u001b[39mUser_ID \u001b[38;5;241m==\u001b[39m user_id]\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m emb_list \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bert_embeddings_mean_from_series\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_series\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_user\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_emb_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(emb_list\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    110\u001b[0m df_emb \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(emb_list, columns\u001b[38;5;241m=\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/pesquisa/stance-prediction-UstanceBR/src/features/bert_embedding_methods.py:60\u001b[0m, in \u001b[0;36mcreate_bert_embeddings_mean_from_series\u001b[0;34m(model_name, text_series)\u001b[0m\n\u001b[1;32m     57\u001b[0m tokenized_text, tokens_tensor, segments_tensors \u001b[38;5;241m=\u001b[39m bert_text_preparation(text, tokenizer)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# get embeddings\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m list_token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_bert_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# calculate mean\u001b[39;00m\n\u001b[1;32m     63\u001b[0m bert_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(list_token_embeddings)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/pesquisa/stance-prediction-UstanceBR/src/features/bert_embedding_methods.py:32\u001b[0m, in \u001b[0;36mget_bert_embeddings\u001b[0;34m(tokens_tensor, segments_tensors, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embeddings\u001b[39m(tokens_tensor, segments_tensors, model):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     35\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-stance-pred/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from itertools import product\n",
    "set_seed(42)\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "\n",
    "from bert_embedding_methods import *\n",
    "\n",
    "################ Data Paths ################################\n",
    "path_raw_data = '../../data/raw/'\n",
    "path_processed_data = '../../data/processed/'\n",
    "path_interim_data = '../../data/interim/'\n",
    "check_if_file_exists = True\n",
    "############################################################\n",
    "# settings tsfresh\n",
    "settings = MinimalFCParameters()\n",
    "n_parts = 10\n",
    "\n",
    "################ Options for generation ####################\n",
    "model_names = [\n",
    "    #'neuralmind/bert-large-portuguese-cased', \n",
    "    'neuralmind/bert-base-portuguese-cased'\n",
    "    ]\n",
    "list_target = [\n",
    "    'ig', \n",
    "    # 'bo', \n",
    "    # 'cl', \n",
    "    # 'co', \n",
    "    # 'gl', \n",
    "    # 'lu'\n",
    "    ]\n",
    "list_splits = [\n",
    "    'train', \n",
    "    'test'\n",
    "    ]\n",
    "list_datasets = [\n",
    "    'top_mentioned_timelines',\n",
    "    # 'users'\n",
    "]\n",
    "############################################################\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        data_user_emb = pd.concat([data_user, df_emb], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Texts</th>\n",
       "      <th>id_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>golaço!!!!!!!!!</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>manda geral do time principal embora.</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>@g1_fla manda o lincoln, apenas.</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>pow. tô querendo comprar o manto de 0 ainda.</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>@pmattoscrf tá sem água</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>rapaz... capaz do fagner dar um carrinho na ae...</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>que golaço do bahea!</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>como o zé rafael não foi pra copa?</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>lindo é o maracanã</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>bahia 3x0 estádio</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User_ID Polarity                                              Texts  \\\n",
       "876   r2_ig_4      for                                    golaço!!!!!!!!!   \n",
       "877   r2_ig_4      for              manda geral do time principal embora.   \n",
       "878   r2_ig_4      for                   @g1_fla manda o lincoln, apenas.   \n",
       "879   r2_ig_4      for       pow. tô querendo comprar o manto de 0 ainda.   \n",
       "880   r2_ig_4      for                            @pmattoscrf tá sem água   \n",
       "...       ...      ...                                                ...   \n",
       "1396  r2_ig_4      for  rapaz... capaz do fagner dar um carrinho na ae...   \n",
       "1397  r2_ig_4      for                               que golaço do bahea!   \n",
       "1398  r2_ig_4      for                 como o zé rafael não foi pra copa?   \n",
       "1399  r2_ig_4      for                                 lindo é o maracanã   \n",
       "1400  r2_ig_4      for                                  bahia 3x0 estádio   \n",
       "\n",
       "      id_text  \n",
       "876       876  \n",
       "877       877  \n",
       "878       878  \n",
       "879       879  \n",
       "880       880  \n",
       "...       ...  \n",
       "1396     1396  \n",
       "1397     1397  \n",
       "1398     1398  \n",
       "1399     1399  \n",
       "1400     1400  \n",
       "\n",
       "[525 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texts_emb_1</th>\n",
       "      <th>Texts_emb_2</th>\n",
       "      <th>Texts_emb_3</th>\n",
       "      <th>Texts_emb_4</th>\n",
       "      <th>Texts_emb_5</th>\n",
       "      <th>Texts_emb_6</th>\n",
       "      <th>Texts_emb_7</th>\n",
       "      <th>Texts_emb_8</th>\n",
       "      <th>Texts_emb_9</th>\n",
       "      <th>Texts_emb_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Texts_emb_759</th>\n",
       "      <th>Texts_emb_760</th>\n",
       "      <th>Texts_emb_761</th>\n",
       "      <th>Texts_emb_762</th>\n",
       "      <th>Texts_emb_763</th>\n",
       "      <th>Texts_emb_764</th>\n",
       "      <th>Texts_emb_765</th>\n",
       "      <th>Texts_emb_766</th>\n",
       "      <th>Texts_emb_767</th>\n",
       "      <th>Texts_emb_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045000</td>\n",
       "      <td>-0.208693</td>\n",
       "      <td>0.050777</td>\n",
       "      <td>0.496065</td>\n",
       "      <td>0.854287</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.514688</td>\n",
       "      <td>-0.057022</td>\n",
       "      <td>0.258820</td>\n",
       "      <td>-0.118168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260805</td>\n",
       "      <td>0.291028</td>\n",
       "      <td>-0.633723</td>\n",
       "      <td>0.081684</td>\n",
       "      <td>0.281460</td>\n",
       "      <td>-0.581074</td>\n",
       "      <td>0.372463</td>\n",
       "      <td>0.251306</td>\n",
       "      <td>-0.317197</td>\n",
       "      <td>0.019190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025828</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.404067</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.425351</td>\n",
       "      <td>0.370382</td>\n",
       "      <td>0.085218</td>\n",
       "      <td>0.156457</td>\n",
       "      <td>0.059991</td>\n",
       "      <td>0.120229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095615</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>-0.176053</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>0.100416</td>\n",
       "      <td>-0.113554</td>\n",
       "      <td>0.168783</td>\n",
       "      <td>-0.070006</td>\n",
       "      <td>-0.106399</td>\n",
       "      <td>-0.240172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.166182</td>\n",
       "      <td>-0.213030</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>0.019467</td>\n",
       "      <td>0.322564</td>\n",
       "      <td>0.362549</td>\n",
       "      <td>-0.271569</td>\n",
       "      <td>-0.200736</td>\n",
       "      <td>0.073760</td>\n",
       "      <td>-0.213599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133959</td>\n",
       "      <td>-0.178983</td>\n",
       "      <td>-0.528039</td>\n",
       "      <td>-0.147305</td>\n",
       "      <td>0.190063</td>\n",
       "      <td>-0.176582</td>\n",
       "      <td>0.038696</td>\n",
       "      <td>-0.051540</td>\n",
       "      <td>0.146485</td>\n",
       "      <td>-0.346050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.113233</td>\n",
       "      <td>-0.215938</td>\n",
       "      <td>0.233366</td>\n",
       "      <td>0.132885</td>\n",
       "      <td>0.615222</td>\n",
       "      <td>0.381270</td>\n",
       "      <td>0.034493</td>\n",
       "      <td>0.019573</td>\n",
       "      <td>0.169643</td>\n",
       "      <td>-0.170267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030024</td>\n",
       "      <td>0.264813</td>\n",
       "      <td>-0.490721</td>\n",
       "      <td>0.011670</td>\n",
       "      <td>0.105098</td>\n",
       "      <td>-0.404371</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.196532</td>\n",
       "      <td>-0.136261</td>\n",
       "      <td>-0.052261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.113547</td>\n",
       "      <td>-0.259609</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.594648</td>\n",
       "      <td>0.149507</td>\n",
       "      <td>-0.108294</td>\n",
       "      <td>-0.098303</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>-0.308216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014566</td>\n",
       "      <td>-0.073322</td>\n",
       "      <td>-0.558466</td>\n",
       "      <td>0.036730</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>-0.366165</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>-0.166510</td>\n",
       "      <td>-0.004912</td>\n",
       "      <td>-0.507525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>-0.119808</td>\n",
       "      <td>-0.130744</td>\n",
       "      <td>0.169192</td>\n",
       "      <td>0.186540</td>\n",
       "      <td>0.481078</td>\n",
       "      <td>0.137944</td>\n",
       "      <td>-0.104580</td>\n",
       "      <td>-0.017725</td>\n",
       "      <td>0.301596</td>\n",
       "      <td>0.130659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013358</td>\n",
       "      <td>0.189719</td>\n",
       "      <td>-0.503648</td>\n",
       "      <td>-0.319865</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>-0.293383</td>\n",
       "      <td>0.078739</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>-0.079401</td>\n",
       "      <td>-0.112492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.031625</td>\n",
       "      <td>-0.104428</td>\n",
       "      <td>0.215029</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.677144</td>\n",
       "      <td>0.125057</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>-0.031196</td>\n",
       "      <td>0.470381</td>\n",
       "      <td>0.178084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009483</td>\n",
       "      <td>0.287954</td>\n",
       "      <td>-0.261931</td>\n",
       "      <td>-0.186215</td>\n",
       "      <td>0.371542</td>\n",
       "      <td>-0.351913</td>\n",
       "      <td>-0.041769</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>-0.001982</td>\n",
       "      <td>-0.247773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>-0.003893</td>\n",
       "      <td>-0.257727</td>\n",
       "      <td>0.143056</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.792616</td>\n",
       "      <td>-0.028351</td>\n",
       "      <td>-0.089396</td>\n",
       "      <td>0.147133</td>\n",
       "      <td>0.268852</td>\n",
       "      <td>-0.219164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.167698</td>\n",
       "      <td>-0.489168</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>0.523642</td>\n",
       "      <td>-0.453360</td>\n",
       "      <td>0.162436</td>\n",
       "      <td>-0.054795</td>\n",
       "      <td>-0.168814</td>\n",
       "      <td>-0.165745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>0.103024</td>\n",
       "      <td>-0.184562</td>\n",
       "      <td>0.204775</td>\n",
       "      <td>0.160432</td>\n",
       "      <td>0.635338</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>0.090593</td>\n",
       "      <td>-0.041046</td>\n",
       "      <td>0.518083</td>\n",
       "      <td>0.456189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010077</td>\n",
       "      <td>0.409296</td>\n",
       "      <td>-0.415906</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.157779</td>\n",
       "      <td>-0.443966</td>\n",
       "      <td>0.206183</td>\n",
       "      <td>-0.180888</td>\n",
       "      <td>0.071609</td>\n",
       "      <td>-0.240775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>-0.133629</td>\n",
       "      <td>-0.080270</td>\n",
       "      <td>0.322413</td>\n",
       "      <td>0.236020</td>\n",
       "      <td>0.498403</td>\n",
       "      <td>0.195669</td>\n",
       "      <td>0.153748</td>\n",
       "      <td>-0.048611</td>\n",
       "      <td>0.350316</td>\n",
       "      <td>0.424123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240346</td>\n",
       "      <td>0.156888</td>\n",
       "      <td>-0.029175</td>\n",
       "      <td>-0.320164</td>\n",
       "      <td>0.185519</td>\n",
       "      <td>-0.724148</td>\n",
       "      <td>-0.083689</td>\n",
       "      <td>-0.451515</td>\n",
       "      <td>-0.131846</td>\n",
       "      <td>-0.125241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Texts_emb_1  Texts_emb_2  Texts_emb_3  Texts_emb_4  Texts_emb_5  \\\n",
       "0       0.045000    -0.208693     0.050777     0.496065     0.854287   \n",
       "1       0.025828     0.002558     0.404067     0.002754     0.425351   \n",
       "2       0.166182    -0.213030     0.287530     0.019467     0.322564   \n",
       "3       0.113233    -0.215938     0.233366     0.132885     0.615222   \n",
       "4      -0.113547    -0.259609     0.088415     0.004300     0.594648   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "520    -0.119808    -0.130744     0.169192     0.186540     0.481078   \n",
       "521     0.031625    -0.104428     0.215029     0.100024     0.677144   \n",
       "522    -0.003893    -0.257727     0.143056     0.013443     0.792616   \n",
       "523     0.103024    -0.184562     0.204775     0.160432     0.635338   \n",
       "524    -0.133629    -0.080270     0.322413     0.236020     0.498403   \n",
       "\n",
       "     Texts_emb_6  Texts_emb_7  Texts_emb_8  Texts_emb_9  Texts_emb_10  ...  \\\n",
       "0       0.803884     0.514688    -0.057022     0.258820     -0.118168  ...   \n",
       "1       0.370382     0.085218     0.156457     0.059991      0.120229  ...   \n",
       "2       0.362549    -0.271569    -0.200736     0.073760     -0.213599  ...   \n",
       "3       0.381270     0.034493     0.019573     0.169643     -0.170267  ...   \n",
       "4       0.149507    -0.108294    -0.098303     0.246825     -0.308216  ...   \n",
       "..           ...          ...          ...          ...           ...  ...   \n",
       "520     0.137944    -0.104580    -0.017725     0.301596      0.130659  ...   \n",
       "521     0.125057     0.157418    -0.031196     0.470381      0.178084  ...   \n",
       "522    -0.028351    -0.089396     0.147133     0.268852     -0.219164  ...   \n",
       "523     0.185749     0.090593    -0.041046     0.518083      0.456189  ...   \n",
       "524     0.195669     0.153748    -0.048611     0.350316      0.424123  ...   \n",
       "\n",
       "     Texts_emb_759  Texts_emb_760  Texts_emb_761  Texts_emb_762  \\\n",
       "0        -0.260805       0.291028      -0.633723       0.081684   \n",
       "1        -0.095615       0.014377      -0.176053      -0.027519   \n",
       "2        -0.133959      -0.178983      -0.528039      -0.147305   \n",
       "3        -0.030024       0.264813      -0.490721       0.011670   \n",
       "4        -0.014566      -0.073322      -0.558466       0.036730   \n",
       "..             ...            ...            ...            ...   \n",
       "520      -0.013358       0.189719      -0.503648      -0.319865   \n",
       "521      -0.009483       0.287954      -0.261931      -0.186215   \n",
       "522       0.230331       0.167698      -0.489168      -0.041316   \n",
       "523      -0.010077       0.409296      -0.415906       0.055160   \n",
       "524      -0.240346       0.156888      -0.029175      -0.320164   \n",
       "\n",
       "     Texts_emb_763  Texts_emb_764  Texts_emb_765  Texts_emb_766  \\\n",
       "0         0.281460      -0.581074       0.372463       0.251306   \n",
       "1         0.100416      -0.113554       0.168783      -0.070006   \n",
       "2         0.190063      -0.176582       0.038696      -0.051540   \n",
       "3         0.105098      -0.404371       0.057198       0.196532   \n",
       "4         0.240058      -0.366165       0.038901      -0.166510   \n",
       "..             ...            ...            ...            ...   \n",
       "520       0.228900      -0.293383       0.078739       0.169521   \n",
       "521       0.371542      -0.351913      -0.041769      -0.299622   \n",
       "522       0.523642      -0.453360       0.162436      -0.054795   \n",
       "523       0.157779      -0.443966       0.206183      -0.180888   \n",
       "524       0.185519      -0.724148      -0.083689      -0.451515   \n",
       "\n",
       "     Texts_emb_767  Texts_emb_768  \n",
       "0        -0.317197       0.019190  \n",
       "1        -0.106399      -0.240172  \n",
       "2         0.146485      -0.346050  \n",
       "3        -0.136261      -0.052261  \n",
       "4        -0.004912      -0.507525  \n",
       "..             ...            ...  \n",
       "520      -0.079401      -0.112492  \n",
       "521      -0.001982      -0.247773  \n",
       "522      -0.168814      -0.165745  \n",
       "523       0.071609      -0.240775  \n",
       "524      -0.131846      -0.125241  \n",
       "\n",
       "[525 rows x 768 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Texts</th>\n",
       "      <th>id_text</th>\n",
       "      <th>Texts_emb_1</th>\n",
       "      <th>Texts_emb_2</th>\n",
       "      <th>Texts_emb_3</th>\n",
       "      <th>Texts_emb_4</th>\n",
       "      <th>Texts_emb_5</th>\n",
       "      <th>Texts_emb_6</th>\n",
       "      <th>...</th>\n",
       "      <th>Texts_emb_759</th>\n",
       "      <th>Texts_emb_760</th>\n",
       "      <th>Texts_emb_761</th>\n",
       "      <th>Texts_emb_762</th>\n",
       "      <th>Texts_emb_763</th>\n",
       "      <th>Texts_emb_764</th>\n",
       "      <th>Texts_emb_765</th>\n",
       "      <th>Texts_emb_766</th>\n",
       "      <th>Texts_emb_767</th>\n",
       "      <th>Texts_emb_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>golaço!!!!!!!!!</td>\n",
       "      <td>876.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>manda geral do time principal embora.</td>\n",
       "      <td>877.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>@g1_fla manda o lincoln, apenas.</td>\n",
       "      <td>878.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>pow. tô querendo comprar o manto de 0 ainda.</td>\n",
       "      <td>879.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>@pmattoscrf tá sem água</td>\n",
       "      <td>880.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.119808</td>\n",
       "      <td>-0.130744</td>\n",
       "      <td>0.169192</td>\n",
       "      <td>0.186540</td>\n",
       "      <td>0.481078</td>\n",
       "      <td>0.137944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013358</td>\n",
       "      <td>0.189719</td>\n",
       "      <td>-0.503648</td>\n",
       "      <td>-0.319865</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>-0.293383</td>\n",
       "      <td>0.078739</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>-0.079401</td>\n",
       "      <td>-0.112492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031625</td>\n",
       "      <td>-0.104428</td>\n",
       "      <td>0.215029</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.677144</td>\n",
       "      <td>0.125057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009483</td>\n",
       "      <td>0.287954</td>\n",
       "      <td>-0.261931</td>\n",
       "      <td>-0.186215</td>\n",
       "      <td>0.371542</td>\n",
       "      <td>-0.351913</td>\n",
       "      <td>-0.041769</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>-0.001982</td>\n",
       "      <td>-0.247773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003893</td>\n",
       "      <td>-0.257727</td>\n",
       "      <td>0.143056</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.792616</td>\n",
       "      <td>-0.028351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.167698</td>\n",
       "      <td>-0.489168</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>0.523642</td>\n",
       "      <td>-0.453360</td>\n",
       "      <td>0.162436</td>\n",
       "      <td>-0.054795</td>\n",
       "      <td>-0.168814</td>\n",
       "      <td>-0.165745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>-0.184562</td>\n",
       "      <td>0.204775</td>\n",
       "      <td>0.160432</td>\n",
       "      <td>0.635338</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010077</td>\n",
       "      <td>0.409296</td>\n",
       "      <td>-0.415906</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.157779</td>\n",
       "      <td>-0.443966</td>\n",
       "      <td>0.206183</td>\n",
       "      <td>-0.180888</td>\n",
       "      <td>0.071609</td>\n",
       "      <td>-0.240775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133629</td>\n",
       "      <td>-0.080270</td>\n",
       "      <td>0.322413</td>\n",
       "      <td>0.236020</td>\n",
       "      <td>0.498403</td>\n",
       "      <td>0.195669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240346</td>\n",
       "      <td>0.156888</td>\n",
       "      <td>-0.029175</td>\n",
       "      <td>-0.320164</td>\n",
       "      <td>0.185519</td>\n",
       "      <td>-0.724148</td>\n",
       "      <td>-0.083689</td>\n",
       "      <td>-0.451515</td>\n",
       "      <td>-0.131846</td>\n",
       "      <td>-0.125241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     User_ID Polarity                                         Texts  id_text  \\\n",
       "876  r2_ig_4      for                               golaço!!!!!!!!!    876.0   \n",
       "877  r2_ig_4      for         manda geral do time principal embora.    877.0   \n",
       "878  r2_ig_4      for              @g1_fla manda o lincoln, apenas.    878.0   \n",
       "879  r2_ig_4      for  pow. tô querendo comprar o manto de 0 ainda.    879.0   \n",
       "880  r2_ig_4      for                       @pmattoscrf tá sem água    880.0   \n",
       "..       ...      ...                                           ...      ...   \n",
       "520      NaN      NaN                                           NaN      NaN   \n",
       "521      NaN      NaN                                           NaN      NaN   \n",
       "522      NaN      NaN                                           NaN      NaN   \n",
       "523      NaN      NaN                                           NaN      NaN   \n",
       "524      NaN      NaN                                           NaN      NaN   \n",
       "\n",
       "     Texts_emb_1  Texts_emb_2  Texts_emb_3  Texts_emb_4  Texts_emb_5  \\\n",
       "876          NaN          NaN          NaN          NaN          NaN   \n",
       "877          NaN          NaN          NaN          NaN          NaN   \n",
       "878          NaN          NaN          NaN          NaN          NaN   \n",
       "879          NaN          NaN          NaN          NaN          NaN   \n",
       "880          NaN          NaN          NaN          NaN          NaN   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "520    -0.119808    -0.130744     0.169192     0.186540     0.481078   \n",
       "521     0.031625    -0.104428     0.215029     0.100024     0.677144   \n",
       "522    -0.003893    -0.257727     0.143056     0.013443     0.792616   \n",
       "523     0.103024    -0.184562     0.204775     0.160432     0.635338   \n",
       "524    -0.133629    -0.080270     0.322413     0.236020     0.498403   \n",
       "\n",
       "     Texts_emb_6  ...  Texts_emb_759  Texts_emb_760  Texts_emb_761  \\\n",
       "876          NaN  ...            NaN            NaN            NaN   \n",
       "877          NaN  ...            NaN            NaN            NaN   \n",
       "878          NaN  ...            NaN            NaN            NaN   \n",
       "879          NaN  ...            NaN            NaN            NaN   \n",
       "880          NaN  ...            NaN            NaN            NaN   \n",
       "..           ...  ...            ...            ...            ...   \n",
       "520     0.137944  ...      -0.013358       0.189719      -0.503648   \n",
       "521     0.125057  ...      -0.009483       0.287954      -0.261931   \n",
       "522    -0.028351  ...       0.230331       0.167698      -0.489168   \n",
       "523     0.185749  ...      -0.010077       0.409296      -0.415906   \n",
       "524     0.195669  ...      -0.240346       0.156888      -0.029175   \n",
       "\n",
       "     Texts_emb_762  Texts_emb_763  Texts_emb_764  Texts_emb_765  \\\n",
       "876            NaN            NaN            NaN            NaN   \n",
       "877            NaN            NaN            NaN            NaN   \n",
       "878            NaN            NaN            NaN            NaN   \n",
       "879            NaN            NaN            NaN            NaN   \n",
       "880            NaN            NaN            NaN            NaN   \n",
       "..             ...            ...            ...            ...   \n",
       "520      -0.319865       0.228900      -0.293383       0.078739   \n",
       "521      -0.186215       0.371542      -0.351913      -0.041769   \n",
       "522      -0.041316       0.523642      -0.453360       0.162436   \n",
       "523       0.055160       0.157779      -0.443966       0.206183   \n",
       "524      -0.320164       0.185519      -0.724148      -0.083689   \n",
       "\n",
       "     Texts_emb_766  Texts_emb_767  Texts_emb_768  \n",
       "876            NaN            NaN            NaN  \n",
       "877            NaN            NaN            NaN  \n",
       "878            NaN            NaN            NaN  \n",
       "879            NaN            NaN            NaN  \n",
       "880            NaN            NaN            NaN  \n",
       "..             ...            ...            ...  \n",
       "520       0.169521      -0.079401      -0.112492  \n",
       "521      -0.299622      -0.001982      -0.247773  \n",
       "522      -0.054795      -0.168814      -0.165745  \n",
       "523      -0.180888       0.071609      -0.240775  \n",
       "524      -0.451515      -0.131846      -0.125241  \n",
       "\n",
       "[1050 rows x 772 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Texts</th>\n",
       "      <th>id_text</th>\n",
       "      <th>Texts_emb_1</th>\n",
       "      <th>Texts_emb_2</th>\n",
       "      <th>Texts_emb_3</th>\n",
       "      <th>Texts_emb_4</th>\n",
       "      <th>Texts_emb_5</th>\n",
       "      <th>Texts_emb_6</th>\n",
       "      <th>...</th>\n",
       "      <th>Texts_emb_759</th>\n",
       "      <th>Texts_emb_760</th>\n",
       "      <th>Texts_emb_761</th>\n",
       "      <th>Texts_emb_762</th>\n",
       "      <th>Texts_emb_763</th>\n",
       "      <th>Texts_emb_764</th>\n",
       "      <th>Texts_emb_765</th>\n",
       "      <th>Texts_emb_766</th>\n",
       "      <th>Texts_emb_767</th>\n",
       "      <th>Texts_emb_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>golaço!!!!!!!!!</td>\n",
       "      <td>876.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>manda geral do time principal embora.</td>\n",
       "      <td>877.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>@g1_fla manda o lincoln, apenas.</td>\n",
       "      <td>878.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>pow. tô querendo comprar o manto de 0 ainda.</td>\n",
       "      <td>879.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>r2_ig_4</td>\n",
       "      <td>for</td>\n",
       "      <td>@pmattoscrf tá sem água</td>\n",
       "      <td>880.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.119808</td>\n",
       "      <td>-0.130744</td>\n",
       "      <td>0.169192</td>\n",
       "      <td>0.186540</td>\n",
       "      <td>0.481078</td>\n",
       "      <td>0.137944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013358</td>\n",
       "      <td>0.189719</td>\n",
       "      <td>-0.503648</td>\n",
       "      <td>-0.319865</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>-0.293383</td>\n",
       "      <td>0.078739</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>-0.079401</td>\n",
       "      <td>-0.112492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031625</td>\n",
       "      <td>-0.104428</td>\n",
       "      <td>0.215029</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.677144</td>\n",
       "      <td>0.125057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009483</td>\n",
       "      <td>0.287954</td>\n",
       "      <td>-0.261931</td>\n",
       "      <td>-0.186215</td>\n",
       "      <td>0.371542</td>\n",
       "      <td>-0.351913</td>\n",
       "      <td>-0.041769</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>-0.001982</td>\n",
       "      <td>-0.247773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003893</td>\n",
       "      <td>-0.257727</td>\n",
       "      <td>0.143056</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.792616</td>\n",
       "      <td>-0.028351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.167698</td>\n",
       "      <td>-0.489168</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>0.523642</td>\n",
       "      <td>-0.453360</td>\n",
       "      <td>0.162436</td>\n",
       "      <td>-0.054795</td>\n",
       "      <td>-0.168814</td>\n",
       "      <td>-0.165745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>-0.184562</td>\n",
       "      <td>0.204775</td>\n",
       "      <td>0.160432</td>\n",
       "      <td>0.635338</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010077</td>\n",
       "      <td>0.409296</td>\n",
       "      <td>-0.415906</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.157779</td>\n",
       "      <td>-0.443966</td>\n",
       "      <td>0.206183</td>\n",
       "      <td>-0.180888</td>\n",
       "      <td>0.071609</td>\n",
       "      <td>-0.240775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133629</td>\n",
       "      <td>-0.080270</td>\n",
       "      <td>0.322413</td>\n",
       "      <td>0.236020</td>\n",
       "      <td>0.498403</td>\n",
       "      <td>0.195669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240346</td>\n",
       "      <td>0.156888</td>\n",
       "      <td>-0.029175</td>\n",
       "      <td>-0.320164</td>\n",
       "      <td>0.185519</td>\n",
       "      <td>-0.724148</td>\n",
       "      <td>-0.083689</td>\n",
       "      <td>-0.451515</td>\n",
       "      <td>-0.131846</td>\n",
       "      <td>-0.125241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     User_ID Polarity                                         Texts  id_text  \\\n",
       "876  r2_ig_4      for                               golaço!!!!!!!!!    876.0   \n",
       "877  r2_ig_4      for         manda geral do time principal embora.    877.0   \n",
       "878  r2_ig_4      for              @g1_fla manda o lincoln, apenas.    878.0   \n",
       "879  r2_ig_4      for  pow. tô querendo comprar o manto de 0 ainda.    879.0   \n",
       "880  r2_ig_4      for                       @pmattoscrf tá sem água    880.0   \n",
       "..       ...      ...                                           ...      ...   \n",
       "520      NaN      NaN                                           NaN      NaN   \n",
       "521      NaN      NaN                                           NaN      NaN   \n",
       "522      NaN      NaN                                           NaN      NaN   \n",
       "523      NaN      NaN                                           NaN      NaN   \n",
       "524      NaN      NaN                                           NaN      NaN   \n",
       "\n",
       "     Texts_emb_1  Texts_emb_2  Texts_emb_3  Texts_emb_4  Texts_emb_5  \\\n",
       "876          NaN          NaN          NaN          NaN          NaN   \n",
       "877          NaN          NaN          NaN          NaN          NaN   \n",
       "878          NaN          NaN          NaN          NaN          NaN   \n",
       "879          NaN          NaN          NaN          NaN          NaN   \n",
       "880          NaN          NaN          NaN          NaN          NaN   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "520    -0.119808    -0.130744     0.169192     0.186540     0.481078   \n",
       "521     0.031625    -0.104428     0.215029     0.100024     0.677144   \n",
       "522    -0.003893    -0.257727     0.143056     0.013443     0.792616   \n",
       "523     0.103024    -0.184562     0.204775     0.160432     0.635338   \n",
       "524    -0.133629    -0.080270     0.322413     0.236020     0.498403   \n",
       "\n",
       "     Texts_emb_6  ...  Texts_emb_759  Texts_emb_760  Texts_emb_761  \\\n",
       "876          NaN  ...            NaN            NaN            NaN   \n",
       "877          NaN  ...            NaN            NaN            NaN   \n",
       "878          NaN  ...            NaN            NaN            NaN   \n",
       "879          NaN  ...            NaN            NaN            NaN   \n",
       "880          NaN  ...            NaN            NaN            NaN   \n",
       "..           ...  ...            ...            ...            ...   \n",
       "520     0.137944  ...      -0.013358       0.189719      -0.503648   \n",
       "521     0.125057  ...      -0.009483       0.287954      -0.261931   \n",
       "522    -0.028351  ...       0.230331       0.167698      -0.489168   \n",
       "523     0.185749  ...      -0.010077       0.409296      -0.415906   \n",
       "524     0.195669  ...      -0.240346       0.156888      -0.029175   \n",
       "\n",
       "     Texts_emb_762  Texts_emb_763  Texts_emb_764  Texts_emb_765  \\\n",
       "876            NaN            NaN            NaN            NaN   \n",
       "877            NaN            NaN            NaN            NaN   \n",
       "878            NaN            NaN            NaN            NaN   \n",
       "879            NaN            NaN            NaN            NaN   \n",
       "880            NaN            NaN            NaN            NaN   \n",
       "..             ...            ...            ...            ...   \n",
       "520      -0.319865       0.228900      -0.293383       0.078739   \n",
       "521      -0.186215       0.371542      -0.351913      -0.041769   \n",
       "522      -0.041316       0.523642      -0.453360       0.162436   \n",
       "523       0.055160       0.157779      -0.443966       0.206183   \n",
       "524      -0.320164       0.185519      -0.724148      -0.083689   \n",
       "\n",
       "     Texts_emb_766  Texts_emb_767  Texts_emb_768  \n",
       "876            NaN            NaN            NaN  \n",
       "877            NaN            NaN            NaN  \n",
       "878            NaN            NaN            NaN  \n",
       "879            NaN            NaN            NaN  \n",
       "880            NaN            NaN            NaN  \n",
       "..             ...            ...            ...  \n",
       "520       0.169521      -0.079401      -0.112492  \n",
       "521      -0.299622      -0.001982      -0.247773  \n",
       "522      -0.054795      -0.168814      -0.165745  \n",
       "523      -0.180888       0.071609      -0.240775  \n",
       "524      -0.451515      -0.131846      -0.125241  \n",
       "\n",
       "[1050 rows x 772 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stance-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
